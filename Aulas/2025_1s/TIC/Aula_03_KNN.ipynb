{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuilhermePelegrina/Mackenzie/blob/main/Aulas/2025_1s/TIC/Aula_03_KNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0sQAjoQytYV"
      },
      "source": [
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/logo_mackenzie.png'>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYCRqwFdf4Ci"
      },
      "outputs": [],
      "source": [
        "import pandas                  as pd\n",
        "import numpy                   as np\n",
        "import matplotlib.pyplot       as plt\n",
        "import seaborn                 as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpxAuEuifzCj"
      },
      "source": [
        "# **Modelo K-Nearest Neighbor (KNN)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL7lrcXszfEj"
      },
      "source": [
        "Assim como a Regressão Logística, O algoritmo dos K-Vizinhos Mais Próximos (KNN) é um algoritmo de aprendizado de máquina usado para classificação. O KNN classifica cada valor de um conjunto de dados avaliando sua distância em relação aos k vizinhos mais próximos. Se os k vizinhos mais próximos forem majoritariamente de uma classe, a amostra em questão será classificada nesta categoria. A ideia é a seguinte:\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Aulas/Figuras/fig_knn_ex1.png'>\n",
        "\n",
        "Quando o k é pequeno, a classificação fica mais sensível a regiões bem próximas (podendo ocorrer o problema de *overfitting*). Com k grande, a classificação fica menos sujeita a ruídos e pode ser considerada mais robusta. Porém, se k for grande demais, pode ser que haja o problema de *underfitting*.\n",
        "\n",
        "Lembre-se:\n",
        "- *Overfitting*: O modelo tem um ótimo desempenho nos dados de treinamento, mas o resultado nos dados de teste é ruim. O modelo se especializou nos dados de treinamento (não tem capacidade de generalização).\n",
        "\n",
        "- *Underfitting*: O desempenho do modelo já é ruim no próprio treinamento. Na etapa de treinamento, não houve o aprendizado de relações entre as variáveis e resultado pode ser visto como algo aleatório.\n",
        "\n",
        "Em nosso exemplo, veja o que acontece se consideramos 5 vizinhos próximos.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Aulas/Figuras/fig_knn_ex2.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9WYDU0C0Ji-"
      },
      "source": [
        "<font size =5 > Algoritmo KNN </font>\n",
        "\n",
        "1.   Selecione o dado a ser classificado;\n",
        "1.   Meça sua distância (euclidiana) em relação a cada um dos outros dados que já estão classificados (os dados de treinamento);\n",
        "1.   Selecione os primeros k dados com menores distâncias;\n",
        "1.   Verifique as classes (ou categorias) dos dados que tiveram as k menores distâncias e conte a quantidade de vezes que cada classe apareceu;\n",
        "1.   Classifique esse novo dado como pertencente à classe que mais apareceu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuVz0Pu063im"
      },
      "source": [
        "<font size =4 > Funções Distância </font>\n",
        "\n",
        "Existem várias funções distância, mas em geral aplicaremos aqui a distância Euclidiana:\n",
        "\n",
        "$$ d(\\hat{x},x^i) = \\sqrt{ \\sum_{j=1}^n(\\hat{x}_j - x_j^i)^2 },$$\n",
        "\n",
        "onde $\\hat{x} = [x_1, x_2, \\ldots, x_j]$ é o dado a ser classificado e $x^i = [x_1^i, x_2^i, \\ldots, x_j^i]$ é um dado de treinamento. Esse cálculo, então, é feito para todos os $n$ dados de teinamento $i = 1, \\ldots, n$.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFktO4Kfa2V"
      },
      "source": [
        "### Prós\n",
        "* É simples de implementar;\n",
        "* Treinar é mais fácil;\n",
        "* Tem poucos parâmetros.\n",
        "\n",
        "### Limitações\n",
        "* Alto custo de previsão;\n",
        "* Requer o uso de uma função distância.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kcj5Z4W7IAt"
      },
      "source": [
        "## Exemplo\n",
        "Utilizaremos um exemplo bastante simples com o objetivo de facilitar a compreensão do modelo. Neste exemplo, a classificação 'O cliente vai pagar o empréstimo?', que pode ser Sim (o empréstimo foi concedido e pago pelo cliente - classe 1) ou Não (o empréstimo foi concedido e não foi pago pelo cliente - classe 0), é determinada com base em apenas dois atributos: a idade do cliente e o valor do empréstimo.\n",
        "\n",
        "Note que, para essa construir esse classificador, necessitamos de um conjunto de dados que indique clientes que já pagaram ou não um empréstimo obtido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5FQqQbEf7M8"
      },
      "outputs": [],
      "source": [
        "Loans = pd.DataFrame({'Age':[40,35,45,34,45,39,57,60,50,48,58],\n",
        "                      'Loan':[100000,60000,80000,40000,125000,120000,95000,92000,100000,147000,143000],\n",
        "                      'Repayment':[1,1,1,1,1,1,0,0,0,0,0] }) # 1=vai pagar 0=não vai pagar\n",
        "\n",
        "Loans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9fafYYX7wrZ"
      },
      "source": [
        "Qual deve ser a classificação (vai pagar o não o empréstimo) para um cliente com 50 anos e um emprestimo de 132.000? Vamos visualizar os dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7XG2JEKglBu"
      },
      "outputs": [],
      "source": [
        "# Visualizando os dados!\n",
        "\n",
        "sns.scatterplot(data=Loans, x='Age', y='Loan', hue='Repayment')\n",
        "plt.scatter(50, 132000, c='fuchsia',s=60) # Novo valor a ser classificado\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMrylkymjGEE"
      },
      "outputs": [],
      "source": [
        "# Calculando as distâncias\n",
        "\n",
        "dist = []\n",
        "for i in range(len(Loans)):\n",
        "    d = float( np.sqrt( (Loans.iloc[i].Age - 50)**2 + (Loans.iloc[i].Loan - 132000)**2 ) )\n",
        "    dist.append(d)\n",
        "\n",
        "Loans['dist'] = dist\n",
        "\n",
        "Loans.sort_values('dist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDPeWwAjjxMZ"
      },
      "source": [
        "Considerando k=3 (os três vizinhos mais próximos) a classificação para esse novo cliente seria: Vai pagar o empréstimo (*Repayment* = 1).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viNpVrtV8sG4"
      },
      "source": [
        "<font size =5> **Cuidado!** Normalizar os dados </font>\n",
        "\n",
        "A distância euclidiana é sensível à escala dos dados, o que significa que variáveis com escalas diferentes podem afetar o resultado. Para evitar esse problema, é comum normalizar ou padronizar os dados antes de calcular a distância euclidiana. Isso garante que todas as variáveis tenham a mesma influência na medida de distância.\n",
        "\n",
        "Portanto, antes de calcular distâncias euclidianas em conjuntos de dados com variáveis de escalas diferentes, é recomendável normalizar ou padronizar os dados para obter resultados mais robustos e significativos.\n",
        "\n",
        "Há diversas formas de normalizar dados. Aqui, vamos considerar a normalização pelo máximo. Ou seja, para cada atributo (coluna), dividimos os valores pelo máximo dessa mesma coluna. Isso garante que os dados fiquem entre 0 e 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA2bpKJS90Ne"
      },
      "outputs": [],
      "source": [
        "# Normalizando o novo dado\n",
        "\n",
        "print(\"idade:\", 50/Loans.Age.max())\n",
        "print(\"Loan:\", 132000/Loans.Loan.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Grxx4iza9iZs"
      },
      "outputs": [],
      "source": [
        "# Normalizando os dados\n",
        "\n",
        "Loans[\"Age\"] = Loans.Age / Loans.Age.max()\n",
        "Loans[\"Loan\"] = Loans.Loan / Loans.Loan.max()\n",
        "Loans.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmG4SEtO9xh_"
      },
      "outputs": [],
      "source": [
        "# Calculando as distâncias\n",
        "\n",
        "dist = []\n",
        "for i in range(len(Loans)):\n",
        "    d = float( np.sqrt( (Loans.iloc[i].Age - 0.8333)**2 + (Loans.iloc[i].Loan - 0.8979)**2 ) )\n",
        "    dist.append(d)\n",
        "\n",
        "Loans['dist'] = dist\n",
        "\n",
        "Loans.sort_values('dist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhvWjErk_FIX"
      },
      "source": [
        "Considerando k=3 (os três vizinhos mais próximos) a classificação para esse novo cliente seria: Não vai pagar o empréstimo (*Repayment* = 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB65A4g2APNA"
      },
      "source": [
        "## Usando o módulo `neighbors` da biblioteca `sklearn`\n",
        "\n",
        "Vamos agora usar o módulo `neighbors` da biblioteca `sklearn` para implementar o KNN. Também, neste exemplo, vamos considerar uma outra técnica de normalização dos dados. Neste caso, para cada dado de uma mesma coluna, subtraímos a média e dividimos pelo desvio padrão (desta mesma coluna). Como resultados, cada coluna terá média igual a 0 e desvio padrão igual a 1. Essa técnica também é chamada de padronização."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rhJj4eFBNgY"
      },
      "outputs": [],
      "source": [
        "from sklearn import neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzJ7fWzeSW5D"
      },
      "outputs": [],
      "source": [
        "Loans = pd.DataFrame({'Age':[40,35,45,34,45,39,57,60,50,48,58],\n",
        "                      'Loan':[100000,60000,80000,40000,125000,120000,95000,92000,100000,147000,143000],\n",
        "                      'Repayment':[1,1,1,1,1,1,0,0,0,0,0] }) # 1=vai pagar 0=não vai pagar\n",
        "\n",
        "Loans.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAQO6EXTSd56"
      },
      "outputs": [],
      "source": [
        "new_case=pd.DataFrame({'Age':[50],\n",
        "                      'Loan':[132000]}) # 1=vai pagar 0=não vai pagar\n",
        "new_case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBHfAd19ApHJ"
      },
      "outputs": [],
      "source": [
        "# Dados de entrada/saída\n",
        "X = Loans[['Age','Loan']]\n",
        "y = Loans[[\"Repayment\"]]\n",
        "\n",
        "# padroniza os dados para que a média seja 0 e o desvio padrão seja 1.\n",
        "X_padronizados = (X-X.mean())/X.std()\n",
        "\n",
        "# Definindo ou declarando o modelo\n",
        "k=3\n",
        "clf = neighbors.KNeighborsClassifier(k)\n",
        "\n",
        "# Aprendizado (Emprega o conjunto de treinamento)\n",
        "clf.fit(X_padronizados, y)\n",
        "\n",
        "# Fazendo a predição\n",
        "new_case_padronizado=(new_case-X.mean())/X.std()\n",
        "Loan_type_pred = clf.predict(new_case_padronizado)\n",
        "\n",
        "print('Classificação KNN, k=', k ,', para o novo empréstimo é ', Loan_type_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d59LoErVMd0"
      },
      "source": [
        "<font size= 5> Adicionando variáveis categóricas </font>\n",
        "\n",
        "Vamos agora incluir variáveis categóricas no problema de classificação. Nesse caso, vamos incluir uma variável que indica se o empréstimo é de curto prazo (*Short*), longo prazo (*Long*), ou prazo indefinido (*Undefined*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaxAwsgLVUrK"
      },
      "outputs": [],
      "source": [
        "Loans = pd.DataFrame({'Age':[40,35,45,34,45,39,57,60,50,48,58],\n",
        "                      'Loan':[100000,60000,80000,40000,125000,120000,95000,92000,100000,147000,143000],\n",
        "                      'Duration':['Short','Long','Short','Undefined','Long','Short','Long','Short','Undefined','Long', 'Short'],\n",
        "                      'Repayment':[1,1,1,1,1,1,0,0,0,0,0] }) # 1=vai pagar 0=não vai pagar\n",
        "Loans.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTF62FGEVnYO"
      },
      "outputs": [],
      "source": [
        "new_case  = pd.DataFrame({'Age':[50],\n",
        "                      'Duration':['Short'],\n",
        "                      'Loan':[132000]})\n",
        "new_case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nw8W2hB-5Zm"
      },
      "source": [
        "Uma forma de lidar com variáveis categóricas é de \"binarizar\" as categorias, de maneira a criar novas colunas de dados que contenha 1 quando a categoria está presente na amostra e 0 quando não está. Nesse caso, um atributo categórico, com $m$ categorias, é substituído por $m$ novas colunas.\n",
        "\n",
        "Veja o exemplo a seguir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1HTIgucVqkv"
      },
      "outputs": [],
      "source": [
        "# Variáveis dummy (Hot encode)\n",
        "\n",
        "dummies = pd.get_dummies(Loans[\"Duration\"])\n",
        "Loans = pd.concat([Loans, dummies],axis=1)\n",
        "Loans.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf2aHzxIWWGg"
      },
      "outputs": [],
      "source": [
        "# Eliminando a coluna original\n",
        "Loans.drop(columns=['Duration'], inplace=True)\n",
        "Loans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmnnzH40ZDux"
      },
      "outputs": [],
      "source": [
        "# New case\n",
        "\n",
        "dummies_new_case = pd.get_dummies(new_case[\"Duration\"])\n",
        "new_case = pd.concat([new_case, dummies_new_case],axis=1)\n",
        "new_case.drop(columns=['Duration'], inplace=True)\n",
        "new_case.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que como neste exemplo estamos classificando apenas uma amostra (e não um conjunto de teste), aparece apenas uma coluna da variável categórica após a transformação do dado. Então, para que os dados de entrada para esta amostra de teste tenha o mesmo tamanho dos dados de entrada, precisamos adicionar as colunas faltantes."
      ],
      "metadata": {
        "id": "SA7bnsX-CYTN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOEryWFUZcvX"
      },
      "outputs": [],
      "source": [
        "# Adicionando as colunas que faltam\n",
        "new_case['Long'] = 0\n",
        "new_case['Undefined'] = 0\n",
        "new_case"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Atenção*: Na etapa de normalização dos dados, aplicamos a técnica apenas nas variáveis numéricas. Ou seja, não normalizamos as variáveis dummies criadas a partir de dados categóricos."
      ],
      "metadata": {
        "id": "oVZRwAi4JJFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FcvqCJtWc99"
      },
      "outputs": [],
      "source": [
        "# Dados de entrada/saída\n",
        "X = Loans[['Age','Loan']]\n",
        "y = Loans[[\"Repayment\"]]\n",
        "\n",
        "# Normalizando os dados\n",
        "X_padronizados = (X-X.mean())/X.std()\n",
        "\n",
        "# Adicionando dummie (variáveis categóricas) nos dados de treinamento\n",
        "X_padronizados = pd.concat([X_padronizados, dummies],axis=1)\n",
        "\n",
        "# Definindo ou declarando o modelo\n",
        "k=3\n",
        "clf = neighbors.KNeighborsClassifier(k)\n",
        "\n",
        "# Aprendizado (Emprega o conjunto de treinamento)\n",
        "clf.fit(X_padronizados, y)\n",
        "\n",
        "# Fazendo a predição\n",
        "X_new_case=new_case[['Age','Loan']]\n",
        "X_new_case_padr=(X_new_case-X.mean())/X.std()\n",
        "\n",
        "# Adicionando dummie (variáveis categóricas) nos dados de teste\n",
        "X_new_case_padr=pd.concat([X_new_case_padr, new_case[[\"Long\",\"Short\",\"Undefined\"]]],axis=1)\n",
        "\n",
        "Loan_type_pred = clf.predict(X_new_case_padr)\n",
        "\n",
        "print('Classificação Knn, k=', k ,', para o novo empréstimo é', Loan_type_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vbRaeWVW1z5"
      },
      "source": [
        "# Exercício com separação dos dados entre treinamento e teste\n",
        "\n",
        "Neste exercício, usaremos a base de dados Auto MPG. Lembre-se que este conjunto de dados descreve o consumo de diferentes carros (medido pelo número de milhas percorridas com o uso de um galão de gasolina - mgp) com base no seguinte conjunto de características de tais carros:\n",
        "\n",
        "cylinders -> Número de cilíndros.\n",
        "\n",
        "displacement -> Capacidade do motor.\n",
        "\n",
        "horsepower -> Potência (cavalo-vapor).\n",
        "\n",
        "weight -> Peso.\n",
        "\n",
        "acceleration -> Tempo, em segundos, até atingir a velocidade de 100 km/h (partindo do carro em repouso).\n",
        "\n",
        "model_year -> Ano do modelo do carro\n",
        "\n",
        "origin -> Local de produção do carro.\n",
        "\n",
        "name -> Nome do modelo do carro.\n",
        "\n",
        "Para descrição completa dos dados acesse https://archive.ics.uci.edu/ml/datasets/auto+mpg."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6f1vC4ivtyp"
      },
      "source": [
        "**ETAPA 1. Lendo o conjunto de dados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlJv-unJvsCH"
      },
      "outputs": [],
      "source": [
        "# Importando o conjunto de dados\n",
        "\n",
        "df = sns.load_dataset('mpg')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbibV3-Vv3lA"
      },
      "source": [
        "**ETAPA 2. Eliminando dados faltantes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DNlctNJwHwH"
      },
      "outputs": [],
      "source": [
        "# Verificando células vazias\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEl9TjMqv61A"
      },
      "outputs": [],
      "source": [
        "# Removendo células vazias\n",
        "df.dropna(inplace = True)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV48njlxwYfZ"
      },
      "source": [
        "**ETAPA 3. Separando variáveis de entrada (X) e de saída (y)**\n",
        "\n",
        "Vamos considerar *origin* como a variável de saída (**y**, classe a ser predita) e como variáveis de entrada (**X**, variáveis explicativas) todas as demais, com exceção da coluna *name*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOCtPQVpwkSJ"
      },
      "outputs": [],
      "source": [
        "# Dados de entrada\n",
        "X = df.drop(['name','origin'], axis=1)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfBTwnla1fNQ"
      },
      "outputs": [],
      "source": [
        "# Dados de saída\n",
        "y = df[\"origin\"]\n",
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZWVz_bS2t46"
      },
      "source": [
        "**ETAPA 4. Conversão de variáveis categóricas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UCgfud_2t47"
      },
      "outputs": [],
      "source": [
        "# Neste conjunto de dados não há essa necessidade.\n",
        "# Se fosse necessário, extraia as dummies e adicione nos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC9Sr4is3eul"
      },
      "source": [
        "**ETAPA 5. Separação dos dados em treinamento e teste**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaqUHTus3euw"
      },
      "outputs": [],
      "source": [
        "# Separação dos dados\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1) # Poderia não ter random_state ou ser outro número\n",
        "\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnD2TAle2b52"
      },
      "source": [
        "**ETAPA 6. Normalização dos dados (se necessário)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYVaCOd62m3Z"
      },
      "outputs": [],
      "source": [
        "# Normalizando os dados (subtraindo a média e dividindo pelo desvio padrão dos dados de treinamento)\n",
        "X_train_padronizados = (X_train-X_train.mean())/X_train.std()\n",
        "X_test_padronizados = (X_test-X_train.mean())/X_train.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHU7KtQ_2-JD"
      },
      "source": [
        "**ETAPA 7. Declarando o modelo de aprendizado de máquina**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY15O-QN2-Jd"
      },
      "outputs": [],
      "source": [
        "# Definindo ou declarando o modelo\n",
        "from sklearn import neighbors\n",
        "\n",
        "k=3\n",
        "modelo = neighbors.KNeighborsClassifier(k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M--OdYQ3JFe"
      },
      "source": [
        "**ETAPA 8. Treinando o modelo de aprendizado de máquina**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W34KCKcz3JFf"
      },
      "outputs": [],
      "source": [
        "# Definindo ou declarando o modelo com base nos dados de treinamento\n",
        "modelo.fit(X_train_padronizados, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI93GG6c4O63"
      },
      "source": [
        "**ETAPA 9. Fazendo predições nos dados de teste**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq5lj0i64S0h"
      },
      "outputs": [],
      "source": [
        "# Fazendo as predições\n",
        "y_pred = modelo.predict(X_test_padronizados)\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rFvhTTX4pKn"
      },
      "source": [
        "**ETAPA 10. Calculando o desempenho (pela acurácia)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAM7UoBV4pKn"
      },
      "outputs": [],
      "source": [
        "# Construindo a matriz de confusao\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, y_pred, labels=modelo.classes_,normalize='true')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=modelo.classes_)\n",
        "\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "print(cnf_matrix)\n",
        "\n",
        "acertos = np.sum(np.diag(cnf_matrix))\n",
        "\n",
        "print('Porcentagem de acertos : ', acertos/np.sum(cnf_matrix))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.origin.value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "EUWsLjgJdmeh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqN0QjLJAB7hcOAfAFEFS5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}