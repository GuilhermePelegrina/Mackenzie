{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuilhermePelegrina/Mackenzie/blob/main/Aulas/2025_1s/TIC/Aula_05_Arvore_de_Decisao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyXCpdAl9Y9o"
      },
      "source": [
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/logo_mackenzie.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9tDgZeaQxjW"
      },
      "source": [
        "# **Árvore de Decisão**\n",
        "\n",
        "Árvores de decisão são modelos estatísticos que utilizam um treinamento supervisionado para a classificação e previsão de dados. Estes modelos utilizam a estratégia de dividir para conquistar: um problema complexo é decomposto em sub-problemas mais simples e recursivamente esta técnica é aplicada a cada sub-problema.\n",
        "\n",
        "```\n",
        "As árvores de decisão estão entre os mais populares algoritmos de inferência e tem sido aplicado em várias áreas como, por exemplo, diagnóstico médico e\n",
        "risco de crédito.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-W7wbwoRCB3"
      },
      "source": [
        "# Exemplos\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Aulas/Figuras/fig_trees_ex1.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDPRnuzaXj1f"
      },
      "source": [
        "# Exemplo (completo)\n",
        "\n",
        "Para entender como funciona uma árvore de decisão, considere os dados apresentados no *scatterplot* abaixo. Esses dados representam características de 54 motores que estão em bom funcionamento (20 pontos em verde - classe 0) ou com funcionamento irregular (34 pontos em vermelho - classe 1). Como características, temos a temperatura do motor e a rotação por minuto (RPM). Com base nesses dados, então, queremos construir uma inteligência computacional capaz de identificador se um novo motor está ou não em bom funcionamento a partir dessas características (medidas através de sensores, por exemplo).\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Aulas/Figuras/fig_trees_motor1.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWHIuTNwY-uW"
      },
      "source": [
        "A ideia por trás de uma Árvore de Decisão é criar partições no espaço dos atributos de maneira a separar as classes. Então, o critério utilizado para realizar as partições é o que traz uma maior ganho de informação para realizar a classificação dos dados. Assim, como será visto com maior detalhe na sequência, esse ganho de informação está relacionado ao quanto a partição consegue discriminar os dados entre as classes. Escolhe-se como valor do atributo para o corrente nó aquele que resulta no **maior ganho de informação**.\n",
        "\n",
        "Em nosso exemplo, podemos iniciar a árvore de decisão com um primeiro nó particionando os dados de acordo com a Temperatura.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Aulas/Figuras/fig_trees_motor2.png'>\n",
        "\n",
        "Note que, ao particionar os dados, obtemos 3 sub-grupos onde temos diferentes proporções de dados de cada classe. Mas como medir se essa partição foi boa? Para responder a essa pergunta, usamos uma medida de ganho de informação. Mas, antes de calcular o ganho de informação, precisamos medir a impureza dos dados em cada nó. Uma forma de medir tal impureza é a partir da entropia (há outras medidas, como o *Gini*, mas aqui vamos explorar apenas a entropia).\n",
        "\n",
        "**Entropia**\n",
        "\n",
        "A entropia caracteriza a impureza dos dados: é uma medida da falta de homogeneidade dos dados de entrada em relação a sua classificação. Por exemplo, a entropia é máxima (igual a 1) quando o conjunto de dados é heterogêneo (as classes ainda não estão discriminadas). Por outro lado, a entropia é mínima (igual a 0) quando o conjunto de dados é homogênio (as classes estão bem discriminadas).\n",
        "\n",
        "Dado um conjunto de entrada ($s$) que pode ter $c$ classes distintas, a entropia de $s$ será dada por\n",
        "\n",
        "$$ Entropia(s) =  \\sum_i^c -p_i \\times log_2(p_i), $$\n",
        "\n",
        "em que  $p_i$ é a proporção de dados em $s$ que pertencem à classe $i$.\n",
        "\n",
        "Em nosso exemplo, no nó raiz (antes da primeira partição), a entropia é dada por\n",
        "\n",
        "$$ Entropia =  \\left(- \\frac{34}{54} * log_2 \\left(\\frac{34}{54} \\right) \\right) + \\left(- \\frac{20}{54} * log_2 \\left(\\frac{20}{54} \\right) \\right) = 0.4202 + 0.5307 = 0.9509. $$\n",
        "\n",
        "Ou seja, obviamente, os dados ainda não estão separados. Após particionar os dados por $x_1 < 49$, a entropia do nó resultante (nó intermediário) foi\n",
        "\n",
        "$$ Entropia =  -1 * log_2(1) = 0. $$\n",
        "\n",
        "Nesse caso, houve uma separação das classes nesse nó. Por outro lado, o nó resultante de $62 \\geq x_1 \\geq 49$ possui a seguinte entropia:\n",
        "\n",
        "$$ Entropia =  \\left(- \\frac{16}{36} * log_2 \\left(\\frac{16}{36} \\right) \\right) + \\left(- \\frac{20}{36} * log_2 \\left(\\frac{20}{36} \\right) \\right) = 0.5199 + 0.4711 = 0.9910, $$\n",
        "\n",
        "ou seja, os dados ainda não estão bem separados. Após calcular a entropia para todos os nós resultantes de uma partição, podemos determinar o ganho de informação associado.\n",
        "\n",
        "**Ganho de informação**\n",
        "\n",
        "O ganho de informação vai indicar o quanto a partição foi boa. Basicamente, esse cálculo leva em conta se os dados ficaram os não misturados nas partições intermediárias (ou seja, a impureza desses nós).\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Aulas/Figuras/fig_trees_ex2.png'>\n",
        "\n",
        "para o cálculo, avaliamos a entropia do \"nó pai\" (nó que foi feito a partição) e os \"nós filhos\" (nós intermediários) após a partição. A fórmula é dada por:\n",
        "\n",
        "$$GI = Entropia(pai) - \\sum_j^r w_j Entropia(filho_j), $$\n",
        "\n",
        "onde $w_j$ é a proporção dos dados contidos no nó filho $j$.\n",
        "\n",
        "Em nosso exemplo, após a primeira partição, temos seguinte o ganho de informação:\n",
        "\n",
        "$$GI = 0.9509 - \\frac{8}{54} * 0 - \\frac{36}{54} * 0.9910 - \\frac{10}{54} * 0 = 0.2902. $$\n",
        "\n",
        "Note que esse ganho de informação aconteceu em um cenário onde conseguimos separar bem uma das classes (pontos em vermelho) em duas dar partições. Mas, se ao invés dos valores usamos nessa partição, tivéssemos usados outros valores, poderíamos ter alcançado um valor pior de ganho de informação. Lembrando que quanto maior o ganho de informação, melhor foi a escolha da partição.\n",
        "\n",
        "*Para fazer em casa:* Refaça os cálculos assumindo como partição $x_1 < 52$ ao invés de $x_1 < 49$ (quatros pontos em verde ficaria nessa partição).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNvvvDtHhBvL"
      },
      "source": [
        "Seguindo com as partições, como ainda temos um nó com alta impureza, continuamos particionando. Mas, agora, faremos a partição com base na RPM. Após particionar os dados por $x_2 < 5800$, $5800 \\geq x_2 \\geq 7000$ e $x_2 \\geq 7000$, as entropias dos nós resultantes (nós finais) foram todos iguais a 0. Ou seja, houve uma completa separação das classes. Ao calcularmos o ganho de informação, obtemos o valor máximo para essa partição, que no caso, é o mesmo valor de 0.2902.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Aulas/Figuras/fig_trees_motor31.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDerp--7lKxJ"
      },
      "source": [
        "# Atenção!\n",
        "\n",
        "Neste exemplo simples, conseguimos separar todas as classes perfeitamente. No entanto, isso raramente acontece na prática. Então, para ajustar o modelo de Árvore de Decisão que melhor se adequa aos dados utilizados, procuramos avaliar (de forma a varrer o espaço dos atributos) quais os valores dos atributos usados nas partições que fornecem os maiores ganhos de informação. Para essa varredura, há um alto custo de treinamento.\n",
        "\n",
        "Veja abaixo os pontos positivos e negativos em se usar uma Árvore de Decisão:\n",
        "\n",
        "Prós:\n",
        "\n",
        "* Não é um modelo *black-box*, ou seja, os resultados podem ser interpretados.\n",
        "* É eficiente\n",
        "* Permite trabalhar diretamente com valores categóricos\n",
        "\n",
        "Limitações\n",
        "* Tem um alto custo de treinamento\n",
        "* É sensível a sobreajuste(\\*)\n",
        "\n",
        "(\\*) Para minimizar esse efeito pode-se fazer a *poda* de níveis da Árvore ou emprego de **Random Forests**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIr_4QzOlOOo"
      },
      "source": [
        "## Exercício usando o `sklearn`: Banknote Autentication\n",
        "\n",
        "Agora vamos ver um caso de dados reais. Esse conjunto de dados, chamado [Banknote Autentication](https://archive.ics.uci.edu/dataset/267/banknote+authentication), descreve 1372 cédulas a partir de quatro características extraídas de cada uma delas: *variance*, *skewness*, *curtosis* e *entropy*. Essas características são extraídas das imagens a partir de uma transformação usada em processamento de sinais (*Wavelet Transform*), a qual permite representar uma imagem a partir de medidades numéricas/estatísticas. Além dessas características, também temos a informação se cada cédula é verdadeira (classe 1) ou falsa (classe 0).\n",
        "\n",
        "Veja na sequência uma descrição desse conjunto de dados.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjY0h7A-nimI"
      },
      "outputs": [],
      "source": [
        "# Importando bibliotecas\n",
        "\n",
        "import pandas                  as pd\n",
        "#import numpy                   as np\n",
        "#import matplotlib.pyplot       as plt\n",
        "#import seaborn                 as sns\n",
        "\n",
        "#import warnings\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "notes = pd.read_csv(\"https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Datasets/data_banknotes.csv\")\n",
        "notes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1t5vmCDu1a-"
      },
      "outputs": [],
      "source": [
        "# Explorando os dados (númerro de linhas e colunas)\n",
        "notes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W16_tW8lu1bK"
      },
      "outputs": [],
      "source": [
        "# Explorando os dados (classes)\n",
        "notes.authentic.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9NgH64bu1bK"
      },
      "outputs": [],
      "source": [
        "# Explorando os dados (dados faltantes)\n",
        "notes.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLtU2YOdu1bL"
      },
      "outputs": [],
      "source": [
        "# Explorando os dados (tipo de dados)\n",
        "notes.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VUv4ygNu1bL"
      },
      "source": [
        "### Árvore de Decisão: Treinamento e Teste\n",
        "\n",
        "Lembrando que os conjuntos de Treinamento e Teste são produzidos aleatoriamente. No entanto, ao definir o `seed` (semente de geração aleatória), garantimos a mesma separação para diferentes execuções do comando. Ou seja, nesse caso, garantimos a reprodutibilidade das execuções."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf9g0qSLu1bL"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Preparando os dados\n",
        "X = notes.drop(columns=['authentic'])\n",
        "y = notes['authentic']\n",
        "\n",
        "# Separando os dados de Treinamento e Teste\n",
        "seed = 1984\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=seed)\n",
        "\n",
        "# Declarando o Modelo\n",
        "clf = DecisionTreeClassifier(criterion='entropy',random_state=seed)\n",
        "#clf = DecisionTreeClassifier(criterion='entropy',min_samples_leaf=5,min_samples_split=5,max_depth=None,random_state=seed)\n",
        "\n",
        "# Aprendizado\n",
        "clf.fit(X_train, y_train)                  # Emprega o conjunto de treinamento\n",
        "y_pred = clf.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUnbv-H1u1bL"
      },
      "source": [
        "### Avaliando o ajuste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x7c8pSMu1bM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB5j5VG3u1bM"
      },
      "outputs": [],
      "source": [
        "# Comparando os valores observados com os preditos\n",
        "pd.DataFrame({\"Valores observados\":y_test,\n",
        "              \"Valores preditos\": y_pred})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5w4k19Cu1bM"
      },
      "outputs": [],
      "source": [
        "# Matriz de confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwApFqQvu1bM"
      },
      "outputs": [],
      "source": [
        "# Acuracia\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R7ZuCEzu1bN"
      },
      "outputs": [],
      "source": [
        "# classification_report (exibe outras medidas de desempenho de um classificador)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVwKAw2Ru1bN"
      },
      "source": [
        "### Exibindo a Árvore\n",
        "\n",
        "Não vamos trabalhar com o gráfico das Árvores. Mas deixamos o código aqui para que você empregue se tiver interesse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymCrZGYNu1bN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import tree\n",
        "\n",
        "plt.figure(figsize=(20,12))\n",
        "\n",
        "tree.plot_tree(clf, fontsize=9)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDuKzDRPu1bN"
      },
      "source": [
        "### Opções código!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FBS7DNOu1bN"
      },
      "source": [
        "- **min_samples_leaf** (Número Mínimo de Amostras em uma Folha):\n",
        "Se o número de amostras em uma folha for menor do que o valor definido em **min_samples_leaf**, então a divisão (split) não será realizada e a folha será considerada pura.\n",
        "\n",
        "- **min_samples_split** (Número Mínimo de Amostras para Divisão):\n",
        "número mínimo de amostras necessárias para realizar uma divisão em um nó (um ponto onde a árvore se ramifica).\n",
        "Se o número de amostras em um nó for menor do que o valor definido em min_samples_split, então a divisão não será realizada, e esse nó se tornará uma folha (caso contrário, a árvore continuará dividindo).\n",
        "\n",
        "- **max_depth** (Profundidade Máxima da Árvore):\n",
        "Define o número máximo de níveis ou camadas que a árvore de decisão pode ter a partir do nó raiz (o topo da árvore).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6va3YaG3xywX"
      },
      "source": [
        "## Exercício usando o `sklearn`: Heart Disease\n",
        "\n",
        "Vamos aplicar a árvore de decisão nos dados de predição de doença cardiovascular. Veja, novamente, os dados abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97RVu_cDfkAD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dados = pd.read_csv('https://raw.githubusercontent.com/GuilhermePelegrina/Mackenzie/main/Datasets/data_heart_statlog_cleveland.csv')\n",
        "dados.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EV8AkCbzGtN"
      },
      "source": [
        "Note que, embora todas as informações sejam numéricas, há atributos que são categóricos. E se não tratarmos tais atributos como categóricos, isso traz um problema para o modelo, uma vez que ele vai considerar uma ordem de grandeza entre esses atributos. Não só ordem de grandeza, mas a relação de ordem entre eles. *chest pain type*, por exemplo, possui 4 categorias: `typical` (1), `typical angina` (2), `non-anginal pain` (3) e `asymptomatic` (4). E dizer que uma categoria possui valor 1 e outra valor 3 não tem muito sentido, tanto numéricos quano de ordenamento (um maior que o outro). Veja abaixo uma descrição completa dos dados:\n",
        "\n",
        "1. Age:Patients Age in years (Numeric)\n",
        "\n",
        "2. Sex:Gender of patient (Male - 1, Female - 0) (Nominal)\n",
        "\n",
        "3. Chest Pain Type:Type of chest pain experienced by patient categorized into 1 typical, 2 typical angina, 3 non- anginal pain, 4 asymptomatic (Nominal)\n",
        "\n",
        "4. resting bp s:Level of blood pressure at resting mode in mm/HG (Numerical)\n",
        "\n",
        "5. cholestrol:Serum cholestrol in mg/dl (Numeric)\n",
        "\n",
        "6. fasting blood sugar:Blood sugar levels on fasting > 120 mg/dl represents as 1 in case of true and 0 as false (Nominal)\n",
        "\n",
        "7. resting ecg:Result of electrocardiogram while at rest are represented in 3 distinct values 0 : Normal 1: Abnormality in ST-T wave 2: Left ventricular hypertrophy (Nominal)\n",
        "\n",
        "8. max heart rate:Maximum heart rate achieved (Numeric)\n",
        "\n",
        "9. exercise angina:Angina induced by exercise 0 depicting NO 1 depicting Yes (Nominal)\n",
        "\n",
        "10. oldpeak:Exercise induced ST-depression in comparison with the state of rest (Numeric)\n",
        "\n",
        "11. ST slope:ST segment measured in terms of slope during peak exercise 0: Normal 1: Upsloping 2: Flat 3: Downsloping (Nominal) Target variable:\n",
        "\n",
        "12. target:It is the target variable which we have to predict 1 means patient is suffering from heart risk and 0 means patient is normal.\n",
        "\n",
        "Então, nesse tipo de cenário é necessário tratar essas informações como categóricas. Uma forma de fazer isso e deixar mais clara as informações contidas nos dados consiste em transformar os valores numéricos nas categorias que os mesmos representam. Veja como fica no código abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEyHQSH-pIaA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "sex1, sex2 = dados.sex==1, dados.sex==0\n",
        "dados['sex'] = np.select([sex1, sex2], ['male', 'female'], default=None)\n",
        "\n",
        "pain1, pain2, pain3, pain4 = dados['chest pain type']==1, dados['chest pain type']==2, dados['chest pain type']==3, dados['chest pain type']==4\n",
        "dados['chest pain type'] = np.select([pain1, pain2, pain3, pain4], ['typical', 'typical angina', 'non-anginal pain', 'asymptomatic'], default=None)\n",
        "\n",
        "ecg1, ecg2, ecg3 = dados['resting ecg']==0, dados['resting ecg']==1, dados['resting ecg']==2\n",
        "dados['resting ecg'] = np.select([ecg1, ecg2, ecg3], ['Normal', 'Abnormality', 'Hypertrophy'], default=None)\n",
        "\n",
        "exercise1, exercise2 = dados['exercise angina']==1, dados['exercise angina']==0\n",
        "dados['exercise angina'] = np.select([exercise1, exercise2], ['male', 'female'], default=None)\n",
        "\n",
        "slope1, slope2, slope3, slope4 = dados['ST slope']==0, dados['ST slope']==1, dados['ST slope']==2, dados['ST slope']==3\n",
        "dados['ST slope'] = np.select([slope1, slope2, slope3, slope4], ['Normal', 'Upsloping', 'Flat', 'Downsloping'], default=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq80xHVQxXyD"
      },
      "outputs": [],
      "source": [
        "dados.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdl1tyX90T46"
      },
      "source": [
        "Agora podemos tratar esse dado da maneira correta e aplicar a Árvore de Decisão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X8KEgNr0dN9"
      },
      "outputs": [],
      "source": [
        "# Preparando os dados\n",
        "X = dados.drop(columns=['target'])\n",
        "y = dados['target']\n",
        "\n",
        "# Lidando com variáveis categóricas\n",
        "X = pd.get_dummies(X) # Dessa forma, já transformamos as dummies em categóricas no próprio DataFrame\n",
        "\n",
        "X.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-Mgsn2x2h6r"
      },
      "outputs": [],
      "source": [
        "# Separando os dados de Treinamento e Teste\n",
        "seed = 1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=seed)\n",
        "\n",
        "# Declarando o Modelo\n",
        "clf = DecisionTreeClassifier(criterion='entropy',min_samples_leaf=5,min_samples_split=5,max_depth=None,random_state=5)\n",
        "\n",
        "# Aprendizado\n",
        "clf.fit(X_train, y_train)                  # Emprega o conjunto de treinamento\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwRv1E3bnXIndibTD9gQxI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}