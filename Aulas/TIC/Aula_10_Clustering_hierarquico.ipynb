{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuilhermePelegrina/Mackenzie/blob/main/Aulas/TIC/Aula_10_Clustering_hierarquico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gglUriLmo1sR"
      },
      "source": [
        "<head>\n",
        "  <meta name=\"author\" content=\"Rogério de Oliveira\">\n",
        "  <meta institution=\"author\" content=\"Universidade Presbiteriana Mackenzie\">\n",
        "</head>\n",
        "\n",
        "<img src=\"http://meusite.mackenzie.br/rogerio/mackenzie_logo/UPM.2_horizontal_vermelho.jpg\" width=300, align=\"left\">\n",
        "<!-- <h1 align=left><font size = 6, style=\"color:rgb(200,0,0)\"> optional title </font></h1> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06noRfWHp0of"
      },
      "source": [
        "# Clustering Hierárquico\n",
        "É uma técnica de mineração de dados que agrupa observações (dados) em clusters com base em sua similaridade seguindo uma estrutura hierárquica.\n",
        "Existem dois tipos de clustering hierárquico: aglomerativo e divisivo.\n",
        "\n",
        "> **Aglomerativo**: Esta é uma abordagem \"de baixo para cima\", cada observação começa como um cluster e, em seguida, é agrupado com outras observações com base em sua similaridade, formando clusters cada vez maiores até que todas as observações estejam em um único cluster.\n",
        "\n",
        "> **Divisivo**: Esta é uma abordagem \"de cima para baixo\", todas as observações são agrupadas em um único cluster, e são divididas em clusters menores com base em suas diferenças até que cada observação esteja em seu próprio cluster.\n",
        "\n",
        "Os resultados do agrupamento hierárquico podem ser então apresentados em um **dendrograma**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BW54Jz0o3Fo"
      },
      "source": [
        "<head>\n",
        "  <meta name=\"author\" content=\"Rogério de Oliveira\">\n",
        "  <meta institution=\"author\" content=\"Universidade Presbiteriana Mackenzie\">\n",
        "</head>\n",
        "\n",
        "<img src=\"http://www.orlandoalbarracin.com.br/phyton/dendograma0.png\" width=500, align=\"left\">\n",
        "<!-- <h1 align=left><font size = 6, style=\"color:rgb(200,0,0)\"> optional title </font></h1> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWug180dF0-I"
      },
      "source": [
        ">  **Definindo o número de Clusters**\n",
        "\n",
        "O número de clusters é calculado a partir de um *ponto* de corte no dendograma que determina a distância máxima que os elementos terão dentro de um agrupamento. Mais adiante vamos discutir um critério para definir o número \"ideal\" de clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxSBZhBrA2G2"
      },
      "source": [
        "<head>\n",
        "  <meta name=\"author\" content=\"Rogério de Oliveira\">\n",
        "  <meta institution=\"author\" content=\"Universidade Presbiteriana Mackenzie\">\n",
        "</head>\n",
        "\n",
        "<img src=\"http://www.orlandoalbarracin.com.br/phyton/dendograma_3.png\" width=550, align=\"left\">\n",
        "<!-- <h1 align=left><font size = 6, style=\"color:rgb(200,0,0)\"> optional title </font></h1> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy_mwaDMstKh"
      },
      "source": [
        "### Exemplo - Vamos agrupar um conjunto de dados!\n",
        "\n",
        "Vamos gerar 8 dados para entender como é feito um dendograma. Aqui cada dado terá duas informações, você pode pensar que cada dado contêm, por exemplo, a renda e a idade de certo usuário. **Não estamos interessados em discutir o código** e sim a construção do dendrograma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQN6sMELRnLq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O-bzonup8BE"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "\n",
        "# Cria um conjunto de dados hipotético\n",
        "X = np.array([[1, 1], [1, 0], [2, 5], [2, 6], [5, 8], [3, 10],[5,1],[5,2]])\n",
        "\n",
        "# Calcula as distâncias entre os pontos usando a distância euclidiana\n",
        "Z = linkage(X, metric='euclidean')\n",
        "\n",
        "# Plota o dendograma\n",
        "plt.figure(figsize=(5, 3))\n",
        "\n",
        "plt.title('Dendograma Aglomerativo')\n",
        "plt.ylabel('Distância')\n",
        "dendrogram(Z, leaf_rotation=90, leaf_font_size=10,\n",
        "           labels=[\"[1,1]\",\"[1,0]\",\"[2,5]\",\"[2,6]\",\"[5,8]\",\"[3,10]\",\"[5,1]\",\"[5,2]\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMjXc50vsSTI"
      },
      "source": [
        "#### **Como foi construído o dendograma?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjHOMPEMttfu"
      },
      "source": [
        "1. Primeiro é mensurada a similaridade dos dados calculando uma distância, por exemplo, a euclidiana."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs-S6IsTuBEt"
      },
      "source": [
        "*   Dado 1: [1,1]\n",
        "*   Dado 2: [1,0]  \n",
        "*   Dado 3: [2,5]\n",
        "*   Dado 4: [2,6]\n",
        "*   Dado 5: [5,8]\n",
        "*   Dado 6: [3,10]\n",
        "*   Dado 7: [5,1]\n",
        "*   Dado 8: [5,2]\n",
        "\n",
        "\n",
        "Distância euclidiana entre o Dado 1 e Dado 2:\n",
        "$$ d_{12}=\\sqrt{(1-1)^2+(1-0)^2}=1$$\n",
        "\n",
        "Distância euclidiana entre o Dado 1 e Dado 3:\n",
        "$$ d_{12}=\\sqrt{(1-2)^2+(1-5)^2} =4,123106$$\n",
        "\n",
        "A seguir, apresentam-se todas as distâncias calculadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2gA9-Scr6YS"
      },
      "source": [
        "<head>\n",
        "  <meta name=\"author\" content=\"Rogério de Oliveira\">\n",
        "  <meta institution=\"author\" content=\"Universidade Presbiteriana Mackenzie\">\n",
        "</head>\n",
        "\n",
        "<img src=\"http://www.orlandoalbarracin.com.br/phyton/dendograma_1.png\" width=550, align=\"left\">\n",
        "<!-- <h1 align=left><font size = 6, style=\"color:rgb(200,0,0)\"> optional title </font></h1> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EqVh0lgwHaW"
      },
      "source": [
        "Os dados são agrupados em pares, usando como critério, a menor distância. Note que agora temos 4 clusters\n",
        "\n",
        "*   Cluster 1: [1,1] e [1,0]\n",
        "*   Cluster 2: [2,5] e [2,6]\n",
        "*   Cluster 3: [5,8] e [3,10]\n",
        "*   Cluster 4: [5,1] 2 [5,2]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOjNJ_3hwHUv"
      },
      "source": [
        "2. Para continuar como o agrupamento, vamos procurar as seguintes duas menores distâncias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fPq-O4exzNV"
      },
      "source": [
        "<head>\n",
        "  <meta name=\"author\" content=\"Rogério de Oliveira\">\n",
        "  <meta institution=\"author\" content=\"Universidade Presbiteriana Mackenzie\">\n",
        "</head>\n",
        "\n",
        "<img src=\"http://www.orlandoalbarracin.com.br/phyton/dendograma_2.png\" width=550, align=\"left\">\n",
        "<!-- <h1 align=left><font size = 6, style=\"color:rgb(200,0,0)\"> optional title </font></h1> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDnbslSQx1ls"
      },
      "source": [
        "Assim,\n",
        "*   Os clusters que tem os valores [1,1] e [5,1] devem ser agrupados em um único cluster (distância de 4).\n",
        "     *   O novo cluster terá 4 dados: [1,1], [1,0], [5,1] e [5,2]\n",
        "*   Os clusters que tem os valores [2,6] e [5,8] devem ser agrupados em um único cluster (distância de 3,6055).\n",
        "     *   O novo cluster terá 4 dados: [2,5], [2,6], [5,8] e [3,10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw-9esoWy2-0"
      },
      "source": [
        "Note que a seguinte menor distância é 4,1231 que juntaria os valores [1,1] e [2,5], ou seja, cria-se um novo cluster que junta todos os oito valores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqvyqVpz4Oho"
      },
      "source": [
        "## Características do Clustering Hierárquico\n",
        "\n",
        "A seguir, vamos discutir duas características deste método:\n",
        "*   A **função distância** a ser implementada\n",
        "*   Como definir a distância entre um ponto e um cluster **(Linkage)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGd3EeT84Oe_"
      },
      "source": [
        "### Funções Distância\n",
        "A distância euclidiana é a mais aplicadam em modelos *knn*, *kmeans*, mas algumas encontram maior uso em contextos específicos como a distância de **Hamming** para dados binários ou a distância **cosseno** para análise de dados de documentos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cHDZb_Clhxy"
      },
      "source": [
        "Algumas funções distância empregadas em Ciência de Dados:\n",
        "\n",
        "*   Distância euclidiana ${\\displaystyle \\| ab \\| _ {2} = {\\sqrt {\\sum_{i} (a_ {i} -b_ {i}) ^ {2}}}} $\n",
        "\n",
        "*   Distância euclidiana quadrada ${\\displaystyle \\| ab \\| _ {2} ^ {2} = \\sum _ {i} (a_ {i} -b_ {i}) ^ {2}} $\n",
        "\n",
        "*   Distância de Manhattan ${\\displaystyle \\| ab \\| _ {1} = \\sum _ {i} | a_ {i} -b_ {i} |}$\n",
        "\n",
        "*   Distância máxima ${\\displaystyle \\| ab \\| _ {\\infty} = \\max _ {i} | a_ {i} -b_ {i} |}$\n",
        "\n",
        "*   Distância de Mahalanobis ${\\displaystyle {\\sqrt {(ab) ^ {\\top} S ^ {- 1} (ab)}}}$  onde S é a matriz de covariância\n",
        "\n",
        "Outras distâncias:\n",
        "\n",
        "*   Distância de Hamming para Strings\n",
        "\n",
        "*   Distância Cosseno para análise de dados com representação vetorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoOB3qZ6GrtU"
      },
      "source": [
        "### Linkage\n",
        "\n",
        "A função distância está bem definida para distância de dois elementos. Mas ainda não definimos a distância de um elemento a um grupo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtUfutz934bL"
      },
      "source": [
        "Alguns critérios de ligação comumente empregados são:\n",
        "\n",
        "1. Complete linkage\n",
        "\n",
        "$$d(A,B) = \\{\\max \\, d (a, b): a \\in A, \\, b \\in B \\, \\}$$\n",
        "\n",
        "2. Single linkage\n",
        "\n",
        "$$d(A,B) = \\{\\min \\, d (a, b): a \\in A, \\, b \\in B \\, \\}$$\n",
        "\n",
        "3. Average linkage\n",
        "\n",
        "$$d(A,B) = {\\displaystyle {\\frac {1} {| A | \\cdot | B |}} \\sum _ {a \\in A} \\sum _ {b \\in B} d (a, b)} $$\n",
        "\n",
        "\n",
        "Importante notar que o tipo de linkage empregado altera a formação dos agrupamentos além de ter impacto sobre o desempenho do processamento.\n",
        "\n",
        "No exemplo mostrado acima foi considerada single linkage, vamos ver a diferença entre os agrupamentos usando complete linkage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCXJs6Y66nvH"
      },
      "outputs": [],
      "source": [
        "# Cria um conjunto de dados hipotético\n",
        "X = np.array([[1, 1], [1, 0], [2, 5], [2, 6], [5, 8], [3, 10],[5,1],[5,2]])\n",
        "\n",
        "# Calcula as distâncias entre os pontos usando a distância euclidiana\n",
        "Z_single = linkage(X,method='single', metric='euclidean')\n",
        "Z_complete = linkage(X,method='complete', metric='euclidean')\n",
        "\n",
        "\n",
        "# Plota o dendograma\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "ax1.set_title('Dendograma (Linkage Single)')\n",
        "dendrogram(Z_single, ax=ax1, leaf_rotation=90, leaf_font_size=10,\n",
        "           labels=[\"[1,1]\",\"[1,0]\",\"[2,5]\",\"[2,6]\",\"[5,8]\",\"[3,10]\",\"[5,1]\",\"[5,2]\"])\n",
        "\n",
        "ax2.set_title('Dendograma (Linkage Complete)')\n",
        "dendrogram(Z_complete, ax=ax2, leaf_rotation=90, leaf_font_size=10,\n",
        "           labels=[\"[1,1]\",\"[1,0]\",\"[2,5]\",\"[2,6]\",\"[5,8]\",\"[3,10]\",\"[5,1]\",\"[5,2]\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIr_4QzOlOOo"
      },
      "source": [
        "## Análise de dados reais\n",
        "Caso: **Us Arrests**\n",
        "\n",
        "O conjunto de dados [US Arrests](https://www.kaggle.com/code/aishu2218/us-arrests-using-hierarchical-clustering-analysis) contém estatísticas de prisões por 100.000 residentes, sendo assassinato, assalto e estupro. Essas estatísticas foram extraídas dos 50 estados dos EUA. Além dessas estatísticas, também é fornecida a porcentagem da população que vive em áreas urbanas.\n",
        "\n",
        "O intuito ao usar esse conjunto de dados é de agrupar estados que possuem uma certa similaridade para, então, estabelecer políticas específicas para cada grupo obtido.\n",
        "\n",
        "Para ler o conjunto de dados, use o link abaixo:\n",
        "\n",
        "https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Datasets/data_usarrests.csv\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPYeoP83sXy7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Datasets/data_usarrests.csv', index_col=0)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBX27Gews1Di"
      },
      "source": [
        "> **Normalizando os dados**\n",
        "\n",
        "Podemos usar a função `preprocessing.scale`, por exemplo. Há outras funções de normalização. Veja [aqui](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) outras opções."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Vk1x5-SuEFy"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import scale\n",
        "\n",
        "df_scaled = pd.DataFrame(scale(df), columns=df.columns)\n",
        "df_scaled.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeiT9oXumyn8"
      },
      "source": [
        "### Fazendo um agrupando via **Aglomerative Clustering**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhCo8mzk_DuG"
      },
      "source": [
        "Vamos calcular primeiro o número \"ideal\" de clusters para o conjunto de dados.\n",
        "\n",
        "Para isto, vamos usar a métrica **Silhouette** que indica quão bem um objeto se ajusta ao seu próprio cluster em relação aos outros clusters, seus valores variam entre -1 e 1, em que valores mais próximos de 1 representam agrupamentos melhores e valores negativos indicam que um ponto pode ter sido atribuído ao grupo errado.\n",
        "\n",
        "Mais informação aqui: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC3XfgyT_e4C"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "for n_clusters in range(2,8):\n",
        "  model = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='complete')\n",
        "  model.fit(df_scaled)\n",
        "  print('Média do valor de Silhouette para ', n_clusters , ' clusters: ', silhouette_score(df_scaled, model.labels_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaQLk7kV_BjK"
      },
      "source": [
        "\n",
        "Assim, vamos separar os dados em dois clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1srfzIVBqv7y"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "#clf = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')\n",
        "# clf = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='single')\n",
        "clf = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='average')\n",
        "\n",
        "clf.fit(df_scaled)\n",
        "\n",
        "# Resultados\n",
        "labels = clf.labels_\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78MD04a1UCu0"
      },
      "outputs": [],
      "source": [
        "df['cluster'] = labels\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfSskAEIUNl5"
      },
      "source": [
        "### Vamos fazer o dendograma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mNBDvj2myoC"
      },
      "outputs": [],
      "source": [
        "import scipy.cluster.hierarchy as shc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.title(\"Dendrogram\")\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "dendrogram = shc.dendrogram(shc.linkage(df_scaled, method='average')) # cuidado com o nome\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-_tBv7ZXpPT"
      },
      "source": [
        "### Caracterizando os grupos\n",
        "\n",
        "Vamos analisar cada grupo! Aparentemente, um dos grupos é caracterizado por estados com elevado número de assaltos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ya2VNtVWkS3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "g = pd.DataFrame( df.groupby('cluster').mean() ).reset_index()\n",
        "\n",
        "f, axis = plt.subplots(1,2, figsize=(12,5))\n",
        "\n",
        "sns.barplot(data=g[g.cluster==0].drop(columns='cluster'),ax=axis[0],estimator=\"mean\")\n",
        "axis[0].set_title('Alto número de assaltos')\n",
        "axis[0].set_xticklabels(axis[0].get_xticklabels(), rotation=90, ha=\"right\")\n",
        "axis[0].set_ylim(0, 300)\n",
        "\n",
        "sns.barplot(data=g[g.cluster==1].drop(columns='cluster'),ax=axis[1],estimator=\"mean\")\n",
        "axis[1].set_title('Baixo número de assaltos')\n",
        "axis[1].set_xticklabels(axis[1].get_xticklabels(), rotation=90, ha=\"right\")\n",
        "axis[1].set_ylim(0, 300)\n",
        "\n",
        "plt.suptitle('Caracterizando os grupos',fontsize=18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_mb4cxqVqSK"
      },
      "source": [
        "## Análise de dados reais II\n",
        "\n",
        "Caso: **Us Arrests**\n",
        "\n",
        "Vamos considerar, agora, o `linkage = 'average'`. Vamos considerar 3 clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icokX3arV9ks"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Datasets/data_usarrests.csv', index_col=0)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUhMtfk1VXp5"
      },
      "outputs": [],
      "source": [
        "# Normalização e outras preparações dos dados\n",
        "from sklearn.preprocessing import scale\n",
        "df_scaled = pd.DataFrame(scale(df), columns=df.columns)\n",
        "\n",
        "# Avalia o número de agrupamentos desejado\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "for n_clusters in range(2,8):\n",
        "  clf = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='average')\n",
        "  clf.fit_predict(df_scaled)\n",
        "  labels = clf.labels_\n",
        "  print('Média do valor de Silhouette para ', n_clusters , ' clusters: ', silhouette_score(df_scaled, labels, metric='euclidean'))\n",
        "\n",
        "# Faz a clusterização selecionada\n",
        "\n",
        "## Declara o modelo\n",
        "clf = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='average')\n",
        "\n",
        "## 'Treina' o modelo\n",
        "clf.fit(df_scaled)\n",
        "\n",
        "# Resultados\n",
        "labels = clf.labels_\n",
        "print(labels)\n",
        "\n",
        "# Associando os dados\n",
        "df['cluster'] = labels\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZC9t4ENB17L"
      },
      "source": [
        "### Vamos fazer o dendograma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYLFqS4sB17M"
      },
      "outputs": [],
      "source": [
        "import scipy.cluster.hierarchy as shc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.title(\"Dendrogram\")\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "dendrogram = shc.dendrogram(shc.linkage(df_scaled, method='average')) # cuidado com o nome\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cNyPypfB17N"
      },
      "source": [
        "### Caracterizando os grupos\n",
        "\n",
        "Vamos analisar cada grupo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs7oOkWNB17N"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "g = pd.DataFrame( df.groupby('cluster').mean() ).reset_index()\n",
        "\n",
        "f, axis = plt.subplots(1,3, figsize=(12,5))\n",
        "\n",
        "sns.barplot(data=g[g.cluster==0].drop(columns='cluster'),ax=axis[0],estimator=\"mean\")\n",
        "axis[0].set_title('Crimes - elevado')\n",
        "axis[0].set_xticklabels(axis[0].get_xticklabels(), rotation=90, ha=\"right\")\n",
        "axis[0].set_ylim(0, 300)\n",
        "\n",
        "sns.barplot(data=g[g.cluster==1].drop(columns='cluster'),ax=axis[1],estimator=\"mean\")\n",
        "axis[1].set_title('Crimes - médio, pop. urbana - média')\n",
        "axis[1].set_xticklabels(axis[1].get_xticklabels(), rotation=90, ha=\"right\")\n",
        "axis[1].set_ylim(0, 300)\n",
        "\n",
        "sns.barplot(data=g[g.cluster==2].drop(columns='cluster'),ax=axis[2],estimator=\"mean\")\n",
        "axis[2].set_title('Crimes - baixo, pop. urbana - baixa')\n",
        "axis[2].set_xticklabels(axis[1].get_xticklabels(), rotation=90, ha=\"right\")\n",
        "axis[2].set_ylim(0, 300)\n",
        "\n",
        "plt.suptitle('Caracterizando os grupos',fontsize=18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJTgAd6uc-dl"
      },
      "source": [
        "Para casa: Testem agora para 2 clusters e com outro tipo de normalização (`preprocessing.minmax_scale`, por exemplo)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outro conjunto de dados\n",
        "\n",
        "Base de dados: Iris\n",
        "\n",
        "Vamos considerar, agora, o `linkage = 'average'`."
      ],
      "metadata": {
        "id": "1L9LZ5qowP_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Carregar a base de dados Iris\n",
        "iris_data = load_iris()\n",
        "df_iris = pd.DataFrame(data=iris_data.data, columns=iris_data.feature_names)\n",
        "df_iris"
      ],
      "metadata": {
        "id": "eFXcbxpNwdrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVL_NNq5xQS_"
      },
      "source": [
        "> **Normalizando os dados**\n",
        "\n",
        "Podemos usar a função `preprocessing.minmax_scale`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Wnv0AywxQTJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "df_scaled = pd.DataFrame(minmax_scale(df_iris), columns=df_iris.columns)\n",
        "df_scaled.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrPt67VWw-UG"
      },
      "source": [
        "Métrica **Silhouette**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgI4Gmudw-UQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "for n_clusters in range(2,8):\n",
        "  model = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='complete')\n",
        "  model.fit(df_scaled)\n",
        "  print('Média do valor de Silhouette para ', n_clusters , ' clusters: ', silhouette_score(df_scaled, model.labels_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpZxq_Ucw-UQ"
      },
      "source": [
        "\n",
        "Assim, vamos separar os dados em três clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShXFCbzVw-UQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "clf = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='complete')\n",
        "# clf = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='single')\n",
        "#clf = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='average')\n",
        "\n",
        "clf.fit(df_scaled)\n",
        "\n",
        "# Resultados\n",
        "labels = clf.labels_\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPFRVIJbw-UR"
      },
      "outputs": [],
      "source": [
        "df_iris['cluster'] = labels\n",
        "df_iris.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSPXsKbpw-UR"
      },
      "source": [
        "### Vamos fazer o dendograma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYHszIYWw-UR"
      },
      "outputs": [],
      "source": [
        "import scipy.cluster.hierarchy as shc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.title(\"Dendrogram\")\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "dendrogram = shc.dendrogram(shc.linkage(df_scaled, method='complete')) # cuidado com o nome\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m59cKgrXw-US"
      },
      "source": [
        "### Caracterizando os grupos\n",
        "\n",
        "Vamos analisar cada grupo! Aparentemente, um dos grupos é caracterizado por estados com elevado número de assaltos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-F4Kdgyw-US"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "g = pd.DataFrame( df_iris.groupby('cluster').mean() ).reset_index()\n",
        "\n",
        "f, axis = plt.subplots(1,3, figsize=(12,5))\n",
        "\n",
        "sns.barplot(data=g[g.cluster==0].drop(columns='cluster'),ax=axis[0],estimator=\"mean\")\n",
        "axis[0].set_title('Petal length and width baixos')\n",
        "axis[0].set_xticklabels(axis[0].get_xticklabels(), rotation=90, ha=\"right\")\n",
        "axis[0].set_ylim(0, 10)\n",
        "\n",
        "sns.barplot(data=g[g.cluster==1].drop(columns='cluster'),ax=axis[1],estimator=\"mean\")\n",
        "axis[1].set_title('sepal and petal length altos')\n",
        "axis[1].set_xticklabels(axis[1].get_xticklabels(), rotation=90, ha=\"right\")\n",
        "axis[1].set_ylim(0, 10)\n",
        "\n",
        "sns.barplot(data=g[g.cluster==2].drop(columns='cluster'),ax=axis[2],estimator=\"mean\")\n",
        "axis[2].set_title('Valores intermediários')\n",
        "axis[2].set_xticklabels(axis[2].get_xticklabels(), rotation=90, ha=\"right\")\n",
        "axis[2].set_ylim(0, 10)\n",
        "\n",
        "plt.suptitle('Caracterizando os grupos',fontsize=18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repita o exemplo anterior, com a base de dados Iris, para linkage = average"
      ],
      "metadata": {
        "id": "9HX0heYqzk4g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPuJSaIput1v"
      },
      "source": [
        "## Referências\n",
        "Você pode encontrar mais detalhes sobre esses métodos em:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/clustering.html\n",
        "\n",
        "https://online.stat.psu.edu/stat555/node/85/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NinPOyINHH6w"
      },
      "source": [
        "## Discutindo um pouco mais (opcional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf6WXXCvLJgg"
      },
      "source": [
        "### Comparando os Métodos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v1jyQ6YHcl8"
      },
      "source": [
        "Abaixo uma exploração de como os métodos de Clusterização Partitivo (K-médias), Hierárquico e *DBScan* podem diferenciar.\n",
        "\n",
        "O código abaixo não tem importância para você, foque apenas nos resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wj2fsBA98dKB"
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n",
        "\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import cluster, datasets, mixture\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from itertools import cycle, islice\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# ============\n",
        "# Generate datasets. We choose the size big enough to see the scalability\n",
        "# of the algorithms, but not too big to avoid too long running times\n",
        "# ============\n",
        "n_samples = 1500\n",
        "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
        "                                      noise=.05)\n",
        "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
        "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
        "no_structure = np.random.rand(n_samples, 2), None\n",
        "\n",
        "# Anisotropicly distributed data\n",
        "random_state = 170\n",
        "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
        "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
        "X_aniso = np.dot(X, transformation)\n",
        "aniso = (X_aniso, y)\n",
        "\n",
        "# blobs with varied variances\n",
        "varied = datasets.make_blobs(n_samples=n_samples,\n",
        "                             cluster_std=[1.0, 2.5, 0.5],\n",
        "                             random_state=random_state)\n",
        "\n",
        "# ============\n",
        "# Set up cluster parameters\n",
        "# ============\n",
        "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
        "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
        "                    hspace=.01)\n",
        "\n",
        "plot_num = 1\n",
        "\n",
        "default_base = {'quantile': .3,\n",
        "                'eps': .3,\n",
        "                'damping': .9,\n",
        "                'preference': -200,\n",
        "                'n_neighbors': 10,\n",
        "                'n_clusters': 3,\n",
        "                'min_samples': 20,\n",
        "                'xi': 0.05,\n",
        "                'min_cluster_size': 0.1}\n",
        "\n",
        "datasets = [\n",
        "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
        "                     'quantile': .2, 'n_clusters': 2,\n",
        "                     'min_samples': 20, 'xi': 0.25}),\n",
        "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
        "    (varied, {'eps': .18, 'n_neighbors': 2,\n",
        "              'min_samples': 5, 'xi': 0.035, 'min_cluster_size': .2}),\n",
        "    (aniso, {'eps': .15, 'n_neighbors': 2,\n",
        "             'min_samples': 20, 'xi': 0.1, 'min_cluster_size': .2}),\n",
        "    (blobs, {}),\n",
        "    (no_structure, {})]\n",
        "\n",
        "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
        "    # update parameters with dataset-specific values\n",
        "    params = default_base.copy()\n",
        "    params.update(algo_params)\n",
        "\n",
        "    X, y = dataset\n",
        "\n",
        "    # scale dataset for easier parameter selection\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "\n",
        "    # estimate bandwidth for mean shift\n",
        "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
        "\n",
        "    # connectivity matrix for structured Ward\n",
        "    connectivity = kneighbors_graph(\n",
        "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
        "    # make connectivity symmetric\n",
        "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
        "\n",
        "    # ============\n",
        "    # Create cluster objects\n",
        "    # ============\n",
        "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
        "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
        "    ward = cluster.AgglomerativeClustering(\n",
        "        n_clusters=params['n_clusters'], linkage='ward',\n",
        "        connectivity=connectivity)\n",
        "    spectral = cluster.SpectralClustering(\n",
        "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
        "        affinity=\"nearest_neighbors\")\n",
        "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
        "    optics = cluster.OPTICS(min_samples=params['min_samples'],\n",
        "                            xi=params['xi'],\n",
        "                            min_cluster_size=params['min_cluster_size'])\n",
        "    affinity_propagation = cluster.AffinityPropagation(\n",
        "        damping=params['damping'], preference=params['preference'])\n",
        "    average_linkage = cluster.AgglomerativeClustering(\n",
        "        linkage=\"average\", affinity=\"cityblock\",\n",
        "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
        "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
        "    gmm = mixture.GaussianMixture(\n",
        "        n_components=params['n_clusters'], covariance_type='full')\n",
        "\n",
        "    clustering_algorithms = (\n",
        "        ('MiniBatchKMeans', two_means),\n",
        "        ('AgglomerativeClustering', average_linkage),\n",
        "        ('DBSCAN', dbscan),\n",
        "    )\n",
        "\n",
        "    for name, algorithm in clustering_algorithms:\n",
        "        t0 = time.time()\n",
        "\n",
        "        # catch warnings related to kneighbors_graph\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.filterwarnings(\n",
        "                \"ignore\",\n",
        "                message=\"the number of connected components of the \" +\n",
        "                \"connectivity matrix is [0-9]{1,2}\" +\n",
        "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
        "                category=UserWarning)\n",
        "            warnings.filterwarnings(\n",
        "                \"ignore\",\n",
        "                message=\"Graph is not fully connected, spectral embedding\" +\n",
        "                \" may not work as expected.\",\n",
        "                category=UserWarning)\n",
        "            algorithm.fit(X)\n",
        "\n",
        "        t1 = time.time()\n",
        "        if hasattr(algorithm, 'labels_'):\n",
        "            y_pred = algorithm.labels_.astype(np.int)\n",
        "        else:\n",
        "            y_pred = algorithm.predict(X)\n",
        "\n",
        "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
        "        if i_dataset == 0:\n",
        "            plt.title(name, size=18)\n",
        "\n",
        "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
        "                                             '#f781bf', '#a65628', '#984ea3',\n",
        "                                             '#999999', '#e41a1c', '#dede00']),\n",
        "                                      int(max(y_pred) + 1))))\n",
        "        # add black color for outliers (if any)\n",
        "        colors = np.append(colors, [\"#000000\"])\n",
        "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
        "\n",
        "        plt.xlim(-2.5, 2.5)\n",
        "        plt.ylim(-2.5, 2.5)\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())\n",
        "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
        "                 transform=plt.gca().transAxes, size=15,\n",
        "                 horizontalalignment='right')\n",
        "        plot_num += 1\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMkqEbThDoj6"
      },
      "source": [
        "### Silhoutte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWhxX8EHU4Hu"
      },
      "source": [
        "O código abaixo não tem importância para você, apenas os gráficos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LbJ7FCUNS-D"
      },
      "outputs": [],
      "source": [
        "#@markdown Silhouette analysis for KMeans clustering\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "\n",
        "print(__doc__)\n",
        "\n",
        "# Generating the sample data from make_blobs\n",
        "# This particular setting has one distinct cluster and 3 clusters placed close\n",
        "# together.\n",
        "X = df_scaled\n",
        "\n",
        "range_n_clusters = [2, 3, 4, 5, 6]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, ax1 = plt.subplots(1,1)\n",
        "    fig.set_size_inches(8, 5)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "\n",
        "\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVP8Q-wnBdPu"
      },
      "source": [
        "### Complexidade \\* (*um tópico opcional avançado*)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HRjDPHZHToF"
      },
      "source": [
        "\n",
        "O algoritmo padrão para agrupamento aglomerativo hierárquico tem uma complexidade de tempo de ${\\displaystyle {\\mathcal {O}} (n ^ {3})}$  e requer $ {\\displaystyle \\Omega (n ^ {2})}$ de Memória, o que o torna muito lento até mesmo para conjuntos de dados de tamanhos médios.\n",
        "No entanto, para alguns casos especiais, métodos aglomerativos eficientes ideais (de complexidade ${\\displaystyle {\\mathcal {O}} (n ^ {2})}$ são conhecidos *Single Linkage* para ligação única e *Complete Linkage* para agrupamento de ligação completa. Com um heap, o tempo de execução do caso geral pode ser reduzido para ${\\displaystyle {\\mathcal {O}} (n ^ {2} \\ log  n)}$ , uma melhoria no limite mencionado de ${\\displaystyle {\\mathcal {O}} (n ^ {3})}$ , ao custo de aumentar ainda mais o requisitos de memória. Em muitos casos, a sobrecarga de memória dessa abordagem é muito grande para torná-la utilizável na prática.\n",
        "\n",
        "Exceto para o caso especial de ligação única, nenhum dos algoritmos (exceto pesquisa exaustiva em ${\\displaystyle {\\mathcal {O}} (2 ^ {n})}$ pode ser garantido para encontrar a solução ideal.\n",
        "\n",
        "O agrupamento divisivo com uma pesquisa exaustiva é ${\\displaystyle {\\mathcal {O}} (2 ^ {n})}$ , mas é comum usar heurísticas mais rápidas para escolher divisões, como k-médias."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [
        "cPuJSaIput1v",
        "Qf6WXXCvLJgg",
        "uMkqEbThDoj6",
        "DVP8Q-wnBdPu"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}