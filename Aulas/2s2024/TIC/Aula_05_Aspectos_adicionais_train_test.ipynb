{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuilhermePelegrina/Mackenzie/blob/main/Aulas/2s2024/TIC/Aula_05_Aspectos_adicionais_train_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0sQAjoQytYV"
      },
      "source": [
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/logo_mackenzie.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpxAuEuifzCj"
      },
      "source": [
        "# **Aspectos adicionais sobre treinamento e teste**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta aula discutiremos alguns aspectos adicionais sobre as etapas de treinamento e teste. Mais precisamente, serão abordadas técnicas usadas na definição dos conjuntos de treinamento e teste para criar o modelo computacional e avaliar seu desempenho. Além disso, veremos outras formas de avaliar o modelo obtido, explorando, por exemplo, o conceito de equidade em aprendizado de máquina."
      ],
      "metadata": {
        "id": "JwM7OigeNxU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para esta aula, usaremos a base de dados [COMPAS Recidivism Risk](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), da ProPublica. Nesse [link](https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Datasets/data_compas_projC1_train.csv), há a opção por baixar o conjunto de dados. A base de dados COMPAS contém diversos atributos descrevendo pessoas detidas por crimes. Para cada pessoa, há também a classificação como um indivíduo potencialemnte reincidente ou não."
      ],
      "metadata": {
        "id": "PdO668QTnntV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lendo os dados\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "dados = pd.read_csv('https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Datasets/data_compas_projC1_train.csv')\n",
        "dados.head()"
      ],
      "metadata": {
        "id": "pgeC9soFvuEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL7lrcXszfEj"
      },
      "source": [
        "## Como separar dados em treinamento e teste?\n",
        "\n",
        "Vimos nas aulas passadas métodos usados em problemas de classificação. Nos problemas abordados em sala, após separamos os dados em treinamento e teste, criamos o modelo computacional se baseando nos dados de treinamento e avaliamos o desempenho do mesmo nos dados de teste. Essa técnica é chamada de **Hold-out**. No entanto, ela não fornece uma medida de robustez do modelo. Em outras palavras, não temos informação da sensibilidade do modelo para diferentes combinações de dados de treinamento e teste. Para avaliar esse aspecto, há duas abordagens mais usuais, uma baseada em **simulações do Hold-out** e outra na **Validação Cruzada**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9WYDU0C0Ji-"
      },
      "source": [
        "### Simulações do Hold-out\n",
        "\n",
        "Para avaliar a robustez do modelo para diferentes *splits* dos dados, podemos gerar diversos cenários em que a separação dos dados entre treinamento e teste partiu de sementes diferentes. Para cada cenário, criamos o modelo e avaliamos seu desempenho. Note que, como diversas configurações foram consideradas, podemos extrair informações como o desempenho médio, desvio padrão, e até selecionar o modelo cujo desempenho foi o melhor.\n",
        "\n",
        "Veja nos códigos abaixo como gerar essas simulações e extrair informações adicionais sobre a construção do modelo computacional. Usaremos como exemplo a regressão logística."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando o modelo\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# Preparando os dados\n",
        "dados.dropna(inplace=True) # Removendo NaN\n",
        "X = dados.drop('score_risk', axis=1)\n",
        "y = dados['score_risk']\n",
        "\n",
        "# Convertendo variáveis categóricas em dummies\n",
        "dummies = pd.get_dummies(X)\n",
        "X = pd.concat([X, dummies],axis=1)\n",
        "X.drop(columns=['sex', 'age_cat','race','c_charge_degree'], inplace=True)"
      ],
      "metadata": {
        "id": "29rPApyVHExN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulações aleatórias\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nSimul = 10 # Número de simulações\n",
        "acuracia = [] # Lista vazia para guardar as acurácias\n",
        "\n",
        "melhor_modelo_acuracia = 0\n",
        "acuracia_atual = 0\n",
        "melhor_acuracia = 0\n",
        "\n",
        "for ii in range(nSimul):\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify = y) # Split aleatório\n",
        "  logreg.fit(X_train,y_train) # Treinando o modelo\n",
        "  y_pred = logreg.predict(X_test) # Fazendo predições\n",
        "  cnf_matrix = metrics.confusion_matrix(y_test, y_pred) # Matriz de confusão\n",
        "  acuracia_atual = (cnf_matrix[0][0] + cnf_matrix[1][1])/len(y_pred) # Acurácia do modelo atual\n",
        "\n",
        "  if len(acuracia) == 0:\n",
        "    melhor_modelo_acuracia = logreg\n",
        "    melhor_acuracia = acuracia_atual\n",
        "  else:\n",
        "    if acuracia_atual > melhor_acuracia:\n",
        "      melhor_modelo_acuracia = logreg\n",
        "      melhor_acuracia = acuracia_atual\n",
        "\n",
        "  acuracia.append(acuracia_atual) # Atualiza a lista de acurácias\n",
        "\n",
        "print(acuracia)\n",
        "print(melhor_acuracia)"
      ],
      "metadata": {
        "id": "AnzX_Dq4Gv8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulações com sementes definidas\n",
        "\n",
        "sementes = [1,3,59,59,21] # Semente definidas\n",
        "acuracia = [] # Lista vazia para guardar as acurácias\n",
        "\n",
        "melhor_modelo_acuracia = 0\n",
        "acuracia_atual = 0\n",
        "melhor_acuracia = 0\n",
        "\n",
        "for ii in sementes:\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=ii, stratify = y) # Split aleatório\n",
        "  logreg.fit(X_train,y_train) # Treinando o modelo\n",
        "  y_pred = logreg.predict(X_test) # Fazendo predições\n",
        "  cnf_matrix = metrics.confusion_matrix(y_test, y_pred) # Matriz de confusão\n",
        "  acuracia_atual = (cnf_matrix[0][0] + cnf_matrix[1][1])/len(y_pred) # Acurácia do modelo atual\n",
        "\n",
        "  if len(acuracia) == 0:\n",
        "    melhor_modelo_acuracia = logreg\n",
        "    melhor_acuracia = acuracia_atual\n",
        "  else:\n",
        "    if acuracia_atual > melhor_acuracia:\n",
        "      melhor_modelo_acuracia = logreg\n",
        "      melhor_acuracia = acuracia_atual\n",
        "\n",
        "  acuracia.append(acuracia_atual) # Atualiza a lista de acurácias\n",
        "\n",
        "print(acuracia)\n",
        "print(melhor_acuracia)"
      ],
      "metadata": {
        "id": "vY6wcPGTKSiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuVz0Pu063im"
      },
      "source": [
        "### Validação cruzada de *k* pastas (*k*-fold cross-validation)\n",
        "\n",
        "A validação cruzada é uma outra forma de avaliar o classificador construído (e selecionar o que teve melhor resultado). A ideia consiste em dividir os dados em *k* partes iguais (ex: em *k=5* partes, sendo que cada parte contém 20% dos dados) e avaliar o desempenho dos classificadores alterando quais são os dados de teste. Ou seja, na primeira iteração, 1 primeira das *k* pastas é usada no teste e o restante no treinamento. Na segunda iteração, a segunda pasta é usada no teste e o restante no treinamento. E assim por diante. Com isso, garantimos que não há repetição de dados de teste nas diferentes avaliações do modelo adotado.\n",
        "\n",
        "Veja como fica nos códigos abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold # Importando a biblioteca do k-fold\n",
        "\n",
        "nFold = 5 # Definindo o número de pastas\n",
        "k_fold = KFold(nFold,shuffle=True) # Definindo a estrutura do k-fold\n",
        "\n",
        "acuracia = [] # Lista vazia para guardar as acurácias\n",
        "\n",
        "melhor_modelo_acuracia = 0\n",
        "acuracia_atual = 0\n",
        "melhor_acuracia = 0\n",
        "\n",
        "for ii, (train, test) in enumerate(k_fold.split(X, y, groups=y)): # Nesse loop, as listas train e test indicam os índices das amostras em cada conjunto de dados\n",
        "  X_train, X_test, y_train, y_test = X.iloc[train,:], X.iloc[test,:], y.iloc[train], y.iloc[test] # Define os dados de treinamento e de teste\n",
        "\n",
        "  logreg.fit(X_train,y_train) # Treinando o modelo\n",
        "  y_pred = logreg.predict(X_test) # Fazendo predições\n",
        "  cnf_matrix = metrics.confusion_matrix(y_test, y_pred) # Matriz de confusão\n",
        "  acuracia_atual = (cnf_matrix[0][0] + cnf_matrix[1][1])/len(y_pred) # Acurácia do modelo atual\n",
        "\n",
        "  if len(acuracia) == 0:\n",
        "    melhor_modelo_acuracia = logreg\n",
        "    melhor_acuracia = acuracia_atual\n",
        "  else:\n",
        "    if acuracia_atual > melhor_acuracia:\n",
        "      melhor_modelo_acuracia = logreg\n",
        "      melhor_acuracia = acuracia_atual\n",
        "\n",
        "  acuracia.append(acuracia_atual) # Atualiza a lista de acurácias\n",
        "\n",
        "print(acuracia)\n",
        "print(melhor_acuracia)"
      ],
      "metadata": {
        "id": "OF5_7rDf2SCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Métricas de avaliação de um classificador\n",
        "\n",
        "Até o momento, vimos apenas a acurácia como medida de desempenho de um classificador. A acurácia (ou *accuracy score*) é uma métrica simples que mede a proporção de previsões corretas feitas pelo modelo. É a razão entre o número de previsões corretas e o número total de previsões, ou seja, a porcentagem de acertos. Veja abaixo um código que calcula diretamente a acurácia:"
      ],
      "metadata": {
        "id": "i2BfSaDR3eHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report # Import de várias ferramentas para avaliar classificadores\n",
        "\n",
        "# Vamos criar um novo modelo de regressão logística, com seed = 1 no train_test_split e estratificando em y\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1, stratify = y)\n",
        "logreg.fit(X_train,y_train) # Treinando o modelo\n",
        "y_pred = logreg.predict(X_test) # Fazendo predições\n",
        "\n",
        "# Calcular a accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy Score:\", accuracy)"
      ],
      "metadata": {
        "id": "bu8gjPXX4Qc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A acurácia, por si só, não é suficiente para verificar a capacidade preditiva de um modelo de classificação em muitos casos, especialmente quando os dados são desbalanceados ou quando as classes têm diferentes custos associados a erros de classificação. Existem várias razões pelas quais a acurácia isoladamente pode ser enganosa ou inadequada:\n",
        "\n",
        "1. **Desbalanceamento de Classes**: Em problemas em que as classes são\n",
        "desequilibradas, ou seja, uma classe tem muito mais exemplos do que a outra, a acurácia pode ser enganosa. O modelo pode atingir uma alta acurácia simplesmente prevendo a classe majoritária em quase todos os casos, enquanto falha em identificar a classe minoritária. Nesse caso, a acurácia não reflete a capacidade do modelo de identificar corretamente as amostras da classe minoritária.\n",
        "\n",
        "2. **Custo dos Erros**: Em muitos cenários, cometer um tipo específico de erro pode ser mais crítico do que outro. Por exemplo, em diagnósticos médicos, um falso negativo (não diagnosticar uma doença quando ela está presente) pode ter consequências mais graves do que um falso positivo (diagnosticar erroneamente uma doença). A acurácia não leva em consideração o custo associado a diferentes tipos de erros.\n",
        "\n",
        "3. **Contexto do Problema**: A escolha de métricas de avaliação deve ser guiada pelo contexto do problema. O que é mais importante: minimizar falsos positivos, minimizar falsos negativos, encontrar um equilíbrio entre ambos ou otimizar para outra métrica específica? A acurácia não leva em consideração as nuances do problema.\n",
        "\n",
        "4. **Métricas Personalizadas**: Em alguns casos, métricas personalizadas podem ser mais relevantes do que a acurácia. Por exemplo, em aplicações de detecção de fraudes, você pode criar uma métrica personalizada que leve em consideração o impacto financeiro dos erros de classificação.\n",
        "\n",
        "Portanto, a acurácia é uma métrica valiosa, mas não deve ser usada isoladamente. É importante considerar métricas adicionais que se alinhem com os objetivos e as peculiaridades do problema de classificação em questão, especialmente quando há desequilíbrio de classes ou custos desiguais associados a diferentes tipos de erros. Essas métricas proporcionarão uma compreensão mais completa do desempenho do modelo. A seguir vamos entender como a matriz de confusão pode ajudar neste processo.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A5Sq5L0iLbI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relembrando: Matriz de confusão"
      ],
      "metadata": {
        "id": "tniS_jyG5ZOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred, labels=logreg.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cnf_matrix, display_labels=logreg.classes_)\n",
        "disp.plot()"
      ],
      "metadata": {
        "id": "tKyeWHpS5b0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Portanto, a matriz de confusão fornece uma visão detalhada do desempenho do seu modelo em relação às duas classes e é frequentemente usada para calcular métricas como precisão, recall, F1-Score e outras métricas de avaliação.\n",
        "\n",
        "Com base nesses valores, você pode avaliar o desempenho do seu modelo em relação a diferentes tipos de erros e acertos.Isso pode ser feito pelo classification report."
      ],
      "metadata": {
        "id": "jHYLHY4VR-X-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Report:\n",
        "\n",
        "O relatório de classificação (classification report) fornece uma visão mais detalhada do desempenho do modelo, incluindo métricas como precisão, recall, pontuação F1 e suporte para cada classe.\n",
        "\n"
      ],
      "metadata": {
        "id": "wfIq8I8aBux8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Como calcular ?"
      ],
      "metadata": {
        "id": "G9QC0wiFSDHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerar o relatório de classificação\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "9kfSzncfBxjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Como interpretar?\n",
        "Vou explicar linha por linha o que o código do classification_report faz:"
      ],
      "metadata": {
        "id": "a8eElbDnC3yW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification_report: Esta é a função do scikit-learn que gera um relatório de classificação. Ele leva duas entradas obrigatórias: y_test (valores verdadeiros das classes dos dados que vc separou para treino) e y_pred (valores previstos pelo modelo).\n",
        "\n",
        "**Precisão (Precision)**: A precisão é a proporção de previsões positivas corretas em relação ao total de previsões positivas. É calculada como: $$TP / (TP + FP)$$ onde TP são os verdadeiros positivos e FP são os falsos positivos.\n",
        "\n",
        "**Recall (Revocação ou Sensibilidade)**: O recall é a proporção de previsões positivas corretas em relação ao total de amostras verdadeiramente positivas. É calculado como: $$TP / (TP + FN)$$ onde TP são os verdadeiros positivos e FN são os falsos negativos.\n",
        "\n",
        "**F1-Score**: O F1-Score é a média harmônica entre precisão e recall e fornece uma única métrica que leva em consideração tanto falsos positivos quanto falsos negativos. É calculado como 2 $×$ (Precisão * Recall) / (Precisão + Recall).\n",
        "\n",
        "**Suporte (Support)**: O suporte é o número de amostras verdadeiras de cada classe no conjunto de teste.\n",
        "\n",
        "A saída do classification_report é uma string formatada que exibe essas métricas para cada classe no problema de classificação. A saída inclui as métricas para cada classe, bem como a média ponderada dessas métricas, levando em consideração o número de amostras em cada classe.\n",
        "\n",
        "Vale ressaltar que a definição de qual classe é a positive ou qual é a negativa é relativo. Então, para essa análise, o importante é verificar a relação entre o real e o que foi predito. Não importando qual classe é a positiva e qual é a negativa. Podemos considerar a classe de maior interesse como positiva, por exemplo, e se basear nisso para guiar as análises."
      ],
      "metadata": {
        "id": "PUsGiIhjDH3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Equidade em Aprendizado de Máquina**\n",
        "\n",
        "Além de procurar por melhores desempenhos em modelos de aprendizado de máquina, há também preocupações éticas de seu uso. São frequentes os casos nos quais há disparidades éticas associadas a aspectos sensíveis da população, como gênero, raça, faixa etária, etc.\n",
        "\n",
        "Com o intuito de eliminar tais disparidades, diversas regulações sobre o uso da Inteligência Artificial vem surgindo pelo mundo todo. Via de regra, a recomendação (ou, mais precisamente, a *obrigação*) é que os modelos de intelgência artificial / aprendizado de máquina sejam capazes de lidar com os problemas práticos mas que não gerem impactos negativos (dentre eles, os ligados às disparidades éticas) na população.\n",
        "\n",
        "Para ilustrar esses cenários \"injustos\", vamos avaliar e comparar os desempenhos do classificador, nos dados COMPAS, para brancos e negros.\n",
        "\n",
        "Nos comandos abaixo, há um exemplo com os dados de treinamento. No caso do modelo que foi desenvolvido para o projeto, avalie os desempenhos nos dados de teste."
      ],
      "metadata": {
        "id": "JE1g_p8g7yiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparando a acurácia em relação aos brancos e negros\n",
        "\n",
        "accuracy_brancos = accuracy_score(y_test[X_test['race_Caucasian'] == 1], y_pred[X_test['race_Caucasian'] == 1])\n",
        "print(\"Acurácia para brancos:\", accuracy_brancos)\n",
        "\n",
        "accuracy_negros = accuracy_score(y_test[X_test['race_African-American'] == 1], y_pred[X_test['race_African-American'] == 1])\n",
        "print(\"Acurácia para negros:\", accuracy_negros)"
      ],
      "metadata": {
        "id": "EfLwmqJD7JIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparando o número de indivíduos erroneamente classificados como reincidentes, em relação aos brancos e negros\n",
        "\n",
        "erro_brancos = len(y_test[(X_test['race_Caucasian'] == 1) & (y_test[y_test == -1]) & (y_test[y_pred == 1])])/len(y_test[X_test['race_Caucasian'] == 1])\n",
        "print(\"Erro para brancos:\", erro_brancos)\n",
        "\n",
        "erro_negros = len(y_test[(X_test['race_African-American'] == 1) & (y_test[y_test == -1]) & (y_test[y_pred == 1])])/len(y_test[X_test['race_African-American'] == 1])\n",
        "print(\"Erro para negros:\", erro_negros)\n",
        "\n",
        "print('Diferença:', erro_negros - erro_brancos)"
      ],
      "metadata": {
        "id": "Zzm8wgGN7Pai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparando o número de indivíduos erroneamente classificados como não-reincidentes, em relação aos brancos e negros\n",
        "\n",
        "erro_brancos = len(y_test[(X_test['race_Caucasian'] == 1) & (y_test[y_test == 1]) & (y_test[y_pred == -1])])/len(y_test[X_test['race_Caucasian'] == 1])\n",
        "print(\"Erro para brancos:\", erro_brancos)\n",
        "\n",
        "erro_negros = len(y_test[(X_test['race_African-American'] == 1) & (y_test[y_test == 1]) & (y_test[y_pred == -1])])/len(y_test[X_test['race_African-American'] == 1])\n",
        "print(\"Erro para negros:\", erro_negros)\n",
        "\n",
        "print('Diferença:', erro_brancos - erro_negros)"
      ],
      "metadata": {
        "id": "Voso7Yee8KqR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORbuFOqfbDhA8ZtpU55z+L",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}