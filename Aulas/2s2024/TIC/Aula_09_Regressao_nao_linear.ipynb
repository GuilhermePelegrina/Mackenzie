{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuilhermePelegrina/Mackenzie/blob/main/Aulas/2s2024/TIC/Aula_09_Regressao_nao_linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-T3uQA_jdyc"
      },
      "source": [
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/logo_mackenzie.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Regressão não linear**\n",
        "\n",
        "Vimos na aula passada a regressão linear como um modelo de aprendizado de máquina aplicado em problemas de regressão. Um dos pressupostos era que a relação entre variáveis de entrada e variável de saída fosse linear. Ou seja, para um bom ajuste de modelo, essas variáveis deveriam estar relacionadas de uma forma linear (o umais próxima de linear possível).\n",
        "\n",
        "No entanto, em muitas aplicações as variáveis se relacionam de formas não lineares (quadráticas, exponenciais, etc.). Sendo assim, modelos que levam em conta a não linearidade dos dados tendem a levar a resultados melhores.\n",
        "\n",
        "Nesta aula, veremos modelos não-lineares de regressão. São várias as não linearidades que podemos modelar a partir do uso de funções polinomiais, exponenciais, logarítmicas ou até trigonométricas. A escolha da função depende dos dados sob análise. E, em geral, o objetivo na etapa de ajuste do modelo é o mesmo da regressão linear. Ou seja, procuramos minimizar a soma dos erros quadráticos entre os valores previstos e os valores reais.\n"
      ],
      "metadata": {
        "id": "EGGicFO7pCQ1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MaiUzZ9h7qk"
      },
      "source": [
        "## Exemplo: Auto MPG\n",
        "\n",
        "Nesta aula, usaremos novamente o conjunto de dados Auto MPG. Esse conjunto de dados pode ser extraído diretamente da biblioteca *seaborn*.\n",
        "\n",
        "Relembrando, este conjunto de dados descreve o consumo de diferentes carros (medido pelo número de milhas percorridas com o uso de um galão de gasolina) com base no seguinte conjunto de características de tais carros:\n",
        "\n",
        "- cylinders -> Número de cilíndros.\n",
        "\n",
        "- displacement -> Capacidade do motor.\n",
        "\n",
        "- horsepower -> Potência (cavalo-vapor).\n",
        "\n",
        "- weight -> Peso.\n",
        "\n",
        "- acceleration -> Tempo, em segundos, até atingir a velocidade de 100 km/h (partindo do carro em repouso).\n",
        "\n",
        "- model_year -> Ano do modelo do carro\n",
        "\n",
        "- origin -> Local de produção do carro.\n",
        "\n",
        "- name -> Nome do modelo do carro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ9ljSadh7qm"
      },
      "outputs": [],
      "source": [
        "# Leitura dos dados\n",
        "\n",
        "import seaborn as sns\n",
        "df = sns.load_dataset('mpg')\n",
        "df.dropna(inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw_VY5r4iMmx"
      },
      "source": [
        "## Exemplo Auto MPG\n",
        "\n",
        "Vamos agora aplicar a Regressão Linear Múltipla em um novo conjunto de dados, chamado Auto MPG. Esse conjunto de dados pode ser extraído diretamente da biblioteca *seaborn*.\n",
        "\n",
        "Este conjunto de dados descreve o consumo de diferentes carros (medido pelo número de milhas percorridas com o uso de um galão de gasolina) com base no seguinte conjunto de características de tais carros:\n",
        "\n",
        "- cylinders -> Número de cilíndros.\n",
        "\n",
        "- displacement -> Capacidade do motor.\n",
        "\n",
        "- horsepower -> Potência (cavalo-vapor).\n",
        "\n",
        "- weight -> Peso.\n",
        "\n",
        "- acceleration -> Tempo, em segundos, até atingir a velocidade de 100 km/h (partindo do carro em repouso).\n",
        "\n",
        "- model_year -> Ano do modelo do carro\n",
        "\n",
        "- origin -> Local de produção do carro.\n",
        "\n",
        "- name -> Nome do modelo do carro.\n",
        "\n",
        "Uma característica interessante nesse dado é que ele possui variáveis categóricas, o que traz uma interpretação diferente para o coeficiente associado a elas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1cSpowQiMm0"
      },
      "source": [
        "Vamos observar a dispersão dos dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nznNKi6EiMm0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.pairplot(df,\n",
        "             x_vars= df.drop('mpg', axis=1).columns,\n",
        "             y_vars='mpg',\n",
        "             kind='scatter')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTfKLXTdjLAf"
      },
      "source": [
        "Suponha que queremos utilizar apenas a variável *horsepower* para prever o *mpg*. Visualmente, essas variáveis parecem ter uma relação não linear. Vejamos, primeiramente, como fica o modelo de regressão linear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrmLPr5LjLAf"
      },
      "outputs": [],
      "source": [
        "# Fit the resgression line using 'OLS'\n",
        "import statsmodels.formula.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = sm.ols(formula='mpg ~ horsepower',\n",
        "               data=df).fit()\n",
        "print(model.summary())\n",
        "\n",
        "# Predição\n",
        "df['predicted'] = model.predict(df.horsepower)\n",
        "\n",
        "# Scatterplot com Reta de Regressão\n",
        "\n",
        "plt.scatter(df.horsepower, df.mpg, color='blue', label='Dados Observados')\n",
        "plt.plot(df.horsepower, df.predicted, color='red', label='Reta de Regressão')\n",
        "\n",
        "plt.xlabel('Horsepower')\n",
        "plt.ylabel('MPG')\n",
        "plt.title('Scatterplot com Reta de Regressão')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td0DQTeosUsy"
      },
      "source": [
        "Note um $R^2 = 0.606$ relativamente baixo, além de um ajuste inadequado que pode ser visualizado na figura acima. De fato, parece haver uma relação não linear entre os dados. Vejamos na sequência um podelo que pode melhors e ajustar a esses dados."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Regressão Polinomial**\n",
        "\n",
        "Seja $x$ a variável independente e $y$ a variável dependente. A função polinomial de grau $p$ que associa ambas as variáveis é dada pela seguinte equação:\n",
        "\n",
        "$$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\beta_p x^p + \\epsilon,$$\n",
        "\n",
        "onde $\\beta_0$ é o intercepto e $\\beta_1, \\ldots, \\beta_p$ são os demais parâmetros associados aos dados do modelo.\n",
        "\n",
        "Note que a equação acima possui uma certa similaridade em relação à regressão linear. Em ambos os modelos, não há não linearidade em relação aos parâmetros. Ou seja, tanto na regressão linear quando no modelo polinomial, em termos do modelo de otimização, ambos são lineares nos parâmetros. Basta trocar $x^2$ por $x_2$, $x^3$ por $x_3$, e assim por diante, para que o modelo fique idêntico ao da regressão linear, porém, com mais parâmetros. Sendo assim, podemos realizar uma transformação das variáveis para se ajustar ao modelo polinomial e, em seguida, realizar o treinamento a partir da regressão linear como vimos na aula passada. Vejamos como fazer isso na sequência."
      ],
      "metadata": {
        "id": "bZ36S0l9kv8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos criar um modelo de grau 2, ou seja, com a seguinte função:\n",
        "\n",
        "$$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon.$$\n",
        "\n",
        "No Python, a função `PolynomialFeatures` faz a transformação que precisamos, de acordo com o grau do polinômio (`degree`) que desejamos construir."
      ],
      "metadata": {
        "id": "JNJ0r4xepv8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2,include_bias=False) # Ajuste dos dados para uma função polinomial de grau 2"
      ],
      "metadata": {
        "id": "RCnd5EpbkvEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que utilizamos como parâmetro no código acima `include_bias=False`. Isso impõe ao modelo não criar o vetor de dados associado ao $\\beta_0$. Iremos deixar essa função para o modelo de regressão linear (como veremos mais para frente). Veja como ficam os dados transformados."
      ],
      "metadata": {
        "id": "miEujvfDt40A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_poly = poly.fit_transform(df[['horsepower']])\n",
        "\n",
        "print(X_poly)"
      ],
      "metadata": {
        "id": "h0HyiS7ot3Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['horsepower2']] = X_poly[:, 1].reshape(-1,1) # Cria uma nova coluna com os dados transformados\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "muwxP48AwAUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temos, agora, uma coluna de dados com os valores originais ($x$) e outra coluna com esses valores ao quadrado ($x^2$). Agora, ao construir o modelo de regressão linear, teremos uma relação não linear entre as variáveis."
      ],
      "metadata": {
        "id": "EDloGb1uurhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.ols(formula='mpg ~ horsepower + horsepower2',\n",
        "               data=df).fit()\n",
        "print(model.summary())\n",
        "\n",
        "# Predição\n",
        "df['predicted'] = model.predict(df[['horsepower','horsepower2']])\n",
        "\n",
        "# Scatterplot com Reta de Regressão\n",
        "\n",
        "plt.scatter(df.horsepower, df.mpg, color='blue', label='Dados Observados')\n",
        "plt.plot(df.horsepower, df.predicted, '.', color='red', label='Reta de Regressão')\n",
        "\n",
        "plt.xlabel('Horsepower')\n",
        "plt.ylabel('MPG')\n",
        "plt.title('Scatterplot com Reta de Regressão')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8cK1H1rwuq0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste novo modelo, encontramos um coeficiente de determinação melhor que o anterior. O que faz sentido, pois no pior dos casos, o novo ajuste desconsideraria o coeficiente associado aos valores ao quadrado e manteria a função linear. No entando, como existe a relação não linear entre as variáveis, o coeficiente associado ao termo quadrático teve um valor expressivo.\n",
        "\n",
        "Vale ressaltar que usamos apenas uma variável e o grau do polinômio igual a 2. Claramente, podemos incluir mais variáveis no modelo e usar um grau de polinômio maior. Seja $x_1, x_2, \\ldots, x_n$ as variáveis independentes e $y$ a variável dependente. A função polinomial de grau $p$ que associa ambas as variáveis é dada pela seguinte equação:\n",
        "\n",
        "$$y = \\beta_0 + \\sum_{i=1}^n \\beta_i x_i + \\sum_{i=1}^n \\sum_{j=1}^n \\beta_{i,j} x_i x_j + \\sum_{i=1}^n \\sum_{j=1}^n \\sum_{k=1}^n \\beta_{i,j,k} x_i x_j x_k + \\ldots + \\beta_{1,2, \\ldots, n} x_1 x_2 \\ldots x_n + \\epsilon.$$"
      ],
      "metadata": {
        "id": "VH7oocqbfn33"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}