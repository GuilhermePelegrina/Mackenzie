{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuilhermePelegrina/Mackenzie/blob/main/Aulas/2s2024/TIC/Aula_07_Floresta_Aleatoria.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyXCpdAl9Y9o"
      },
      "source": [
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/logo_mackenzie.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9tDgZeaQxjW"
      },
      "source": [
        "# **Floresta aleatória**\n",
        "\n",
        "Na aula anterior, vimos o método da Árvore de Decisão. Nesse método, percebemos que há uma aleatoriedade nas partições dos nós até convergir para o treinamento final do modelo. Além disso, vimos que uma árvore de decisão é sensível ao *overfitting*. Principalmente se consideramos um grsnde número de partições do espaço de atributos.\n",
        "\n",
        "Uma forma de contornar essa sensibilidade ao *overfitting* e criar um modelo de classificação mais robusto seria treinar diversas árvores de decisão e verificar qual seria a \"classificação média\" ou \"vencedora\" desse conjunto. A Floresta Aleatória é um algoritmo de aprendizado de máquina que usa essa estratégia. Cada árvore é treinada em um subconjunto aleatório dos dados, usando uma estratégia de *bagging*, e o resultado médio das classificações de cada modelo individual leva a um modelo mais robusto e menos sensível ao *overfitting*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-W7wbwoRCB3"
      },
      "source": [
        "# Exemplos\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Aulas/Figuras/fig_random_forest.png'>\n",
        "\n",
        "Fonte: [Random Forest and how it works](https://medium.com/@rdhawan201455/random-forest-and-how-it-works-67f408e43a43)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDPRnuzaXj1f"
      },
      "source": [
        "# Etapas\n",
        "\n",
        "As etapas de uma floresta aleatória que diferem tal método de uma simples árvore de decisão são as seguintes:\n",
        "\n",
        "- *Bagging* (método de reamostragem): A floresta aleatória é composta por várias árvores de decisão. Para treinar cada árvore de decisão e ajustar um modelo mais robusto, dentre os dados separados para treinamento, selecionamos um subconjunto de dados aleatoriamente (não o total dos dados de treinamento). A estratégia utilizada é com reposição, ou seja, a mesma amostra poderá ser selecionada para compor os dados de treinamento da árvore. Além disso, a mesma amostra pode ser selecionada para treinar diferentes árvores de decisão. No entanto, essa seleção aleatória muito provavelmente vai levar a árvores treinadas com dados diferentes. Além disso, neste procedimento, em muitos nós a partição é realizada a partir de atributos que não são necessariamente os melhores para separar os dados. Isto aumenta a chance de todos os atributos serem utilizados na partição. Consequentemente, aumenta-se a diversidade da floresta aleatória.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Aulas/Figuras/fig_random_forest_bagging.png'>\n",
        "\n",
        "Fonte: [O que é e como funciona o algoritmo RandomForest - Didática Tech](https://didatica.tech/o-que-e-e-como-funciona-o-algoritmo-randomforest/)\n",
        "\n",
        "\n",
        "- Aleatoriedade na seleção dos atributos: Outro componente aleatório na floresta aleatória é a determinação, em cada etapa de partição dos dados (nos nós), de quais atributos são considerados para ramificar a árvore. Ou seja, do total de atributos no conjunto de dados, selecionamos aleatoriamente um subconjunto para ser considerado na próxima ramificação da árvore. Esse processo é repetido em cada ramificação. Como resultado, temos árvores bem diferentes, aumentando ainda mais a diversidade da floresta aleatória.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Aulas/Figuras/fig_random_forest_subfeatures.png'>\n",
        "\n",
        "Fonte: [O que é e como funciona o algoritmo RandomForest - Didática Tech](https://didatica.tech/o-que-e-e-como-funciona-o-algoritmo-randomforest/)\n",
        "\n",
        "- *Ensemble* na etapa de classificação: Uma vez treinadas as árvores de decisão, para classificar uma nova amostra, coletamos o resutlado de cada árvore. A amostra será classificada de acordo com a classe vencedora. Esse procedimento é conhecido como *ensemble*. Consiste na estratégia de explorar resultados de modelos diferentes de aprendizado de máquina para extrair um resultado mais robusto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWHIuTNwY-uW"
      },
      "source": [
        "# Vantagens\n",
        "\n",
        "- Robustez: Ao criar um conjunto de árvores de decisão com componentes aleatórios, o *ensemble* se baseia em máquinas treinadas diversas, o que contribui para a robustez da classificação.\n",
        "\n",
        "- Redução de *overfitting*: O uso do *ensemble* também reduz a chance de *overfitting*, uma vez que resultados enviesados em uma única árvore de decisão não terá um grande impacto na determinação da classe pela maioria das árvores do modelo. Ou seja, a floresta aleatória é menos sensível à *outliers* no modelo.\n",
        "\n",
        "- Embora até agora vimos apenas o problema de classificação, uma floresta aleatória também pode ser usada para problemas de regressão.\n",
        "\n",
        "# Desvantagens\n",
        "\n",
        "- Custo computacional. Via de regra, quanto maior o número de árvores de decisão, maior a diversidade e, consequentemente, maior a robustez do modelo. No entanto, lembre-se que cada árvore de decisão precisa ser treinada. Sendo assim, quanto maior o número de árvores consideradas, maior o tempo gasto no treinamento da floresta aleatória. Isso traz uma dificuldade no método, que pode demorar levar muito tempo para treinar o modelo.\n",
        "\n",
        "- Interpretabilidade: Interpretar o resultado de uma classificação se torna mais complicada em um *ensemble* do que em um único modelo. Então, em comparação com uma única árvore de decisão onde conseguimos entender as partições ao longo da árvore, uma floresta aleatória tem essa dificuldade de interpretação. Há muitas árvores (e partições) para serem analisadas ao mesmo tempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIr_4QzOlOOo"
      },
      "source": [
        "## Exercício usando o `sklearn`: Banknote Autentication\n",
        "\n",
        "Agora vamos aplicar a floresta aleatória no mesmo conjunto de dados utilizado na aula sobre árvores de decisão. Esse conjunto de dados, chamado [Banknote Autentication](https://archive.ics.uci.edu/dataset/267/banknote+authentication), descreve 1372 cédulas a partir de quatro características extraídas de cada uma delas: *variance*, *skewness*, *curtosis* e *entropy*. Essas características são extraídas das imagens a partir de uma transformação usada em processamento de sinais (*Wavelet Transform*), a qual permite representar uma imagem a partir de medidades numéricas/estatísticas. Além dessas características, também temos a informação se cada cédula é verdadeira (classe 1) ou falsa (classe 0).\n",
        "\n",
        "Veja na sequência uma descrição desse conjunto de dados.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjY0h7A-nimI"
      },
      "outputs": [],
      "source": [
        "# Importando bibliotecas\n",
        "\n",
        "import pandas                  as pd\n",
        "#import numpy                   as np\n",
        "#import matplotlib.pyplot       as plt\n",
        "#import seaborn                 as sns\n",
        "\n",
        "#import warnings\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "notes = pd.read_csv(\"https://raw.githubusercontent.com/guilhermepelegrina/Mackenzie/main/Datasets/data_banknotes.csv\")\n",
        "notes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1t5vmCDu1a-"
      },
      "outputs": [],
      "source": [
        "# Explorando os dados (númerro de linhas e colunas)\n",
        "notes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W16_tW8lu1bK"
      },
      "outputs": [],
      "source": [
        "# Explorando os dados (classes)\n",
        "notes.authentic.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9NgH64bu1bK"
      },
      "outputs": [],
      "source": [
        "# Explorando os dados (dados faltantes)\n",
        "notes.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLtU2YOdu1bL"
      },
      "outputs": [],
      "source": [
        "# Explorando os dados (tipo de dados)\n",
        "notes.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VUv4ygNu1bL"
      },
      "source": [
        "### Floresta aleatória: Treinamento e Teste\n",
        "\n",
        "Lembrando que os conjuntos de Treinamento e Teste são produzidos aleatoriamente. No entanto, ao definir o `seed` (semente de geração aleatória), garantimos a mesma separação para diferentes execuções do comando. Ou seja, nesse caso, garantimos a reprodutibilidade das execuções."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf9g0qSLu1bL"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Preparando os dados\n",
        "X = notes.drop(columns=['authentic'])\n",
        "y = notes['authentic']\n",
        "\n",
        "# Separando os dados de Treinamento e Teste\n",
        "seed = 1984\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=seed)\n",
        "\n",
        "# Declarando o Modelo\n",
        "clf = RandomForestClassifier(random_state=seed)\n",
        "\n",
        "# Aprendizado\n",
        "clf.fit(X_train, y_train)                  # Emprega o conjunto de treinamento\n",
        "y_pred = clf.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUnbv-H1u1bL"
      },
      "source": [
        "### Avaliando o ajuste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x7c8pSMu1bM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5w4k19Cu1bM"
      },
      "outputs": [],
      "source": [
        "# Matriz de confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwApFqQvu1bM"
      },
      "outputs": [],
      "source": [
        "# Acuracia\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R7ZuCEzu1bN"
      },
      "outputs": [],
      "source": [
        "# classification_report (exibe outras medidas de desempenho de um classificador)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparando com uma única árvore de decisão\n",
        "\n"
      ],
      "metadata": {
        "id": "Ymzrf1M2bYd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='entropy',random_state=seed)\n",
        "\n",
        "# Aprendizado\n",
        "model.fit(X_train, y_train)                  # Emprega o conjunto de treinamento\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Matriz de confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# Acuracia\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "oaPo7t37beEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVwKAw2Ru1bN"
      },
      "source": [
        "### Exibindo o conjunto de árvores de decisão\n",
        "\n",
        "Como a floresta aleatória engloba um conjunto de árvores de decisão, podemos plotar tais árvores para analisar o que foi obtido no treinamento do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymCrZGYNu1bN"
      },
      "outputs": [],
      "source": [
        "# Tree Visualisation\n",
        "from sklearn.tree import export_graphviz\n",
        "#from IPython.display import Image\n",
        "import graphviz\n",
        "\n",
        "for i in range(3):\n",
        "    tree = clf.estimators_[i]\n",
        "    dot_data = export_graphviz(tree,\n",
        "                               feature_names=X_train.columns,\n",
        "                               filled=True,\n",
        "                               max_depth=2, # Comando que indica o tamanho da árvore que quer plotar (número de partições)\n",
        "                               impurity=False,\n",
        "                               proportion=True)\n",
        "    graph = graphviz.Source(dot_data)\n",
        "    display(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDuKzDRPu1bN"
      },
      "source": [
        "### Interpretando o modelo\n",
        "\n",
        "Embora a floresta aleatória é mais complicado de interpretar em comparação com uma árvore de decisão, há um comando no `sklearn` que permite identificar a importância de cada atributo na classificação das amostras com base em um *score* interno ao modelo. Esse *score* é calculado automaticamente na etapa de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria um vetor contendo as importâncias atribuídas a cada atributo na base de dados usada no treinamento\n",
        "feature_importances = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "\n",
        "# Plot\n",
        "feature_importances.plot.bar();"
      ],
      "metadata": {
        "id": "dVqmmjaQgDBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esse resulado nos diz então que o atributo *variance* é o que mais contribui para distinguir (e classificar) as amostras. Por outro lado, *entropy* não tem um impacto tão grande. Essa análise é interessante em um campo de estudo chamado de *Feature Engineering* (ou, mais precisamente, *Feature Selection*), o qual explora o quanto cada atributo contribui no modelo treinado. Caso haja atributos com pouca contribuição, os mesmos podem ser removidos da base de dados sem muito prejuízo para a máquina treinada.\n",
        "\n",
        "Lembre-se: quanto menor o número de atributos, mais simples é o modelo e mais fácil ele pode ser interpretado. Além, é claro, de reduzir o tempo computacional."
      ],
      "metadata": {
        "id": "lFzOorrMhsHm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FBS7DNOu1bN"
      },
      "source": [
        "### Opções código\n",
        "\n",
        "- **n_estimators**: Número de árvores de decisão que serão consideradas no *ensemble*. Lembre-se que quanto maior esse número, maior (geralmente) o desempenho do modelo e maior é tempo computacional.\n",
        "\n",
        "- **max_features** {“sqrt”, “log2”, None}: Número de atributos considerados na seleção do subconjunto para particionar cada nó das árvores. Por padrão, considera-se `sqrt`, ou seja, esse número como a raiz quadrada do número total de atributos.\n",
        "\n",
        "- **min_samples_leaf** (Número Mínimo de Amostras em uma Folha): Se o número de amostras em uma folha para cada árvore de decisão for menor do que o valor definido em **min_samples_leaf**, então a divisão (split) não será realizada e a folha será considerada pura.\n",
        "\n",
        "- **min_samples_split** (Número Mínimo de Amostras para Divisão):\n",
        "número mínimo de amostras necessárias para realizar uma divisão em um nó (um ponto onde a árvore se ramifica) em cada árvore de decisão. Se o número de amostras em um nó for menor do que o valor definido em min_samples_split, então a divisão não será realizada, e esse nó se tornará uma folha (caso contrário, a árvore continuará dividindo).\n",
        "\n",
        "- **max_depth** (Profundidade Máxima da Árvore):\n",
        "Define o número máximo de níveis ou camadas que cada árvore de decisão pode ter a partir do nó raiz (o topo da árvore).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6va3YaG3xywX"
      },
      "source": [
        "## Exercício usando o `sklearn`: Heart Disease\n",
        "\n",
        "Vamos aplicar a árvore de decisão nos dados de predição de doença cardiovascular. Veja, novamente, os dados abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97RVu_cDfkAD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dados = pd.read_csv('https://raw.githubusercontent.com/GuilhermePelegrina/Mackenzie/main/Datasets/data_heart_statlog_cleveland.csv')\n",
        "dados.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EV8AkCbzGtN"
      },
      "source": [
        "Note que, embora todas as informações sejam numéricas, há atributos que são categóricos. E se não tratarmos tais atributos como categóricos, isso traz um problema para o modelo, uma vez que ele vai considerar uma ordem de grandeza entre esses atributos. Não só ordem de grandeza, mas a relação de ordem entre eles. *chest pain type*, por exemplo, possui 4 categorias: `typical` (1), `typical angina` (2), `non-anginal pain` (3) e `asymptomatic` (4). E dizer que uma categoria possui valor 1 e outra valor 3 não tem muito sentido, tanto numéricos quano de ordenamento (um maior que o outro). Veja abaixo uma descrição completa dos dados:\n",
        "\n",
        "1. Age:Patients Age in years (Numeric)\n",
        "\n",
        "2. Sex:Gender of patient (Male - 1, Female - 0) (Nominal)\n",
        "\n",
        "3. Chest Pain Type:Type of chest pain experienced by patient categorized into 1 typical, 2 typical angina, 3 non- anginal pain, 4 asymptomatic (Nominal)\n",
        "\n",
        "4. resting bp s:Level of blood pressure at resting mode in mm/HG (Numerical)\n",
        "\n",
        "5. cholestrol:Serum cholestrol in mg/dl (Numeric)\n",
        "\n",
        "6. fasting blood sugar:Blood sugar levels on fasting > 120 mg/dl represents as 1 in case of true and 0 as false (Nominal)\n",
        "\n",
        "7. resting ecg:Result of electrocardiogram while at rest are represented in 3 distinct values 0 : Normal 1: Abnormality in ST-T wave 2: Left ventricular hypertrophy (Nominal)\n",
        "\n",
        "8. max heart rate:Maximum heart rate achieved (Numeric)\n",
        "\n",
        "9. exercise angina:Angina induced by exercise 0 depicting NO 1 depicting Yes (Nominal)\n",
        "\n",
        "10. oldpeak:Exercise induced ST-depression in comparison with the state of rest (Numeric)\n",
        "\n",
        "11. ST slope:ST segment measured in terms of slope during peak exercise 0: Normal 1: Upsloping 2: Flat 3: Downsloping (Nominal) Target variable:\n",
        "\n",
        "12. target:It is the target variable which we have to predict 1 means patient is suffering from heart risk and 0 means patient is normal.\n",
        "\n",
        "Então, nesse tipo de cenário é necessário tratar essas informações como categóricas. Uma forma de fazer isso e deixar mais clara as informações contidas nos dados consiste em transformar os valores numéricos nas categorias que os mesmos representam. Veja como fica no código abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEyHQSH-pIaA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "sex1, sex2 = dados.sex==1, dados.sex==0\n",
        "dados['sex'] = np.select([sex1, sex2], ['male', 'female'], default=None)\n",
        "\n",
        "pain1, pain2, pain3, pain4 = dados['chest pain type']==1, dados['chest pain type']==2, dados['chest pain type']==3, dados['chest pain type']==4\n",
        "dados['chest pain type'] = np.select([pain1, pain2, pain3, pain4], ['typical', 'typical angina', 'non-anginal pain', 'asymptomatic'], default=None)\n",
        "\n",
        "ecg1, ecg2, ecg3 = dados['resting ecg']==0, dados['resting ecg']==1, dados['resting ecg']==2\n",
        "dados['resting ecg'] = np.select([ecg1, ecg2, ecg3], ['Normal', 'Abnormality', 'Hypertrophy'], default=None)\n",
        "\n",
        "exercise1, exercise2 = dados['exercise angina']==1, dados['exercise angina']==0\n",
        "dados['exercise angina'] = np.select([exercise1, exercise2], ['male', 'female'], default=None)\n",
        "\n",
        "slope1, slope2, slope3, slope4 = dados['ST slope']==0, dados['ST slope']==1, dados['ST slope']==2, dados['ST slope']==3\n",
        "dados['ST slope'] = np.select([slope1, slope2, slope3, slope4], ['Normal', 'Upsloping', 'Flat', 'Downsloping'], default=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq80xHVQxXyD"
      },
      "outputs": [],
      "source": [
        "dados.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdl1tyX90T46"
      },
      "source": [
        "Agora podemos tratar esse dado da maneira correta e aplicar a Árvore de Decisão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X8KEgNr0dN9"
      },
      "outputs": [],
      "source": [
        "# Preparando os dados\n",
        "X = dados.drop(columns=['target'])\n",
        "y = dados['target']\n",
        "\n",
        "# Lidando com variáveis categóricas\n",
        "X = pd.get_dummies(X) # Dessa forma, já transformamos as dummies em categóricas no próprio DataFrame\n",
        "\n",
        "X.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-Mgsn2x2h6r"
      },
      "outputs": [],
      "source": [
        "# Separando os dados de Treinamento e Teste\n",
        "seed = 1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=seed)\n",
        "\n",
        "# Declarando o Modelo\n",
        "clf = RandomForestClassifier(random_state=seed, n_estimators=5, max_depth = 3)\n",
        "\n",
        "# Aprendizado\n",
        "clf.fit(X_train, y_train)                  # Emprega o conjunto de treinamento\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Otimizando o modelo (*Hyperparameter Tuning*)\n",
        "\n",
        "Vimos que há diversas customizações no modelo de floresta aleatória (assim como em outros modelos já vistos nesta disciplina). Em aprendizado de máquina, é comum explorar configurações diferentes do modelo para encontrar melhores desempenhos. Essa busca, otimizando os parâmetros do modelo, é chamada de Refinamento dos Hiperparâmetros (ou *Hyperparameter Tuning*). Vamos ver no código abaixo como fazemos isso com a floresta aleatória (para outros métodos, apenas ajuste o modelo e os elementos que serão otimizados)."
      ],
      "metadata": {
        "id": "3I_a4rvaqMpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import randint # randint(a,b) indica um número aleatório inteiro entre a e b\n",
        "from sklearn.model_selection import RandomizedSearchCV # Função de tuning\n",
        "\n",
        "# Cria os possíveis valores (intervalos) que os parâmetros podem assumir\n",
        "param_dist = {'n_estimators': randint(50,500),\n",
        "              'max_depth': randint(1,20)}\n",
        "\n",
        "# Defini o modelo (sem parâmetros à princípio - apenas o random_state, que poderia ser removido)\n",
        "rf = RandomForestClassifier(random_state = 2)\n",
        "\n",
        "# Use random search to find the best hyperparameters\n",
        "rand_search = RandomizedSearchCV(rf,                                # Modelo definido\n",
        "                                 param_distributions = param_dist,  # Intervalos de busca predefinidos\n",
        "                                 n_iter=10,                          # Número de iterações (modelos avaliados) que selecionará aleatoriamente valores dentro dos intervalos\n",
        "                                 cv=5)                              # Número de folds na validação cruzada\n",
        "\n",
        "# Encontra o melhor modelo dentre os possíveis testados\n",
        "rand_search.fit(X_train, y_train)\n",
        "\n",
        "# Salva como um novo objeto o melhor modelo encontrado\n",
        "best_rf = rand_search.best_estimator_\n",
        "\n",
        "# Imprime os parâmetros que levaram ao melhor modelo\n",
        "print('Hiperparâmetros:',  rand_search.best_params_)"
      ],
      "metadata": {
        "id": "g-8ByVv9sRUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando o desempenho\n",
        "\n",
        "best_rf.fit(X_train, y_train)                  # Emprega o conjunto de treinamento\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "BYCqWsaiuGsp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+bfD+oEzNe3fX/0Y9PQT/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}